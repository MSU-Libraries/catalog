{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>This is the technical user documentation for the Michigan State University Libraries' public catalog system. It describes how to set up and configure the containerized VuFind system.</p> <p>Feel free to reach out to our team with any questions or suggestions you have!</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Technical Overview</li> <li>First Time Setup</li> <li>Harvesting &amp; Importing</li> <li>CI/CD</li> </ul>"},{"location":"#administration","title":"Administration","text":"<ul> <li>Backup &amp; Restore</li> <li>Upgrading</li> <li>VuFind</li> <li>Solr</li> <li>MariaDB</li> <li>Traefik</li> <li>Monitoring App</li> <li>Load Testing</li> <li>Helper Scripts</li> <li>Bots</li> <li>Job Schedules</li> </ul>"},{"location":"#developing","title":"Developing","text":"<ul> <li>Writing &amp; Running Tests</li> <li>Using Core VuFind Only</li> <li>Alphabrowse Internals</li> <li>Documentation</li> </ul>"},{"location":"#processes","title":"Processes","text":"<ul> <li>Communication Processes</li> </ul>"},{"location":"CICD/","title":"CI/CD","text":"<p>This page describes the GitLab CI/CD pipeline that is used by the MSU Libraries team to deploy the Docker services as multiple stacks in a multi-node Docker swarm cluster.</p> <p>The <code>catalog-preview</code> branch represents the staging environment for changes before they will be deployed to the production environments.</p> <p>The <code>main</code> branch represents the stable production environment. Branches with the prefix of <code>review-</code> or <code>devel-</code> will create separate stacks on the cluster, auto-provisioning DNS CNAMES as part of the pipeline.</p> <p>The workflow for developers will be to make code changes on <code>devel-</code> environments, then merge them in to the <code>catalog-preview</code> branch, and once a semester (approximately), we will merge that branch into the <code>main</code> branch to deploy to production. Occasionally there may be changes that need to go to production sooner, in that case they will be merged to from the <code>devel-</code> branches to BOTH the <code>catalog-preview</code> and the <code>main</code> branch.</p>"},{"location":"CICD/#environment-based-on-branch-name","title":"Environment based on branch name","text":"<ul> <li> <p>branch: <code>main</code>, stack prefix: <code>catalog-beta</code> (<code>catalog-beta-catalog</code>,   <code>catalog-beta-solr</code>, <code>catalog-beta-mariadb</code>, etc.), URL: https://catalog-beta.lib.msu.edu   (local DNS C-Record to catalog.aws.lib.msu.edu) stack prefix: <code>catalog-prod</code> (<code>catalog-prod-catalog</code>,   <code>catalog-prod-solr</code>, <code>catalog-prod-mariadb</code>, etc.), URL: https://catalog.lib.msu.edu   (local DNS C-Record to catalog.aws.lib.msu.edu)</p> </li> <li> <p>branch: <code>catalog-preview</code>, stack prefix: <code>catalog-preview</code> (<code>catalog-preview-catalog</code>,   <code>catalog-preview-solr</code>, <code>catalog-preview-mariadb</code>, etc.), URL: https://catalog-preview.lib.msu.edu   (local DNS C-Record to catalog.aws.lib.msu.edu)</p> </li> <li> <p>branch: <code>review-some-feature</code>, stack prefix: <code>review-some-feature</code> (<code>review-some-feature-catalog</code>,   <code>review-some-feature-solr</code>, <code>review-some-feature-mariadb</code>, etc.), URL: https://review-some-feature.aws.lib.msu.edu</p> </li> <li> <p>branch: <code>devel-some-feature</code>, stack prefix: <code>devel-some-feature</code> (<code>devel-some-feature-catalog</code>,   <code>devel-some-feature-solr</code>, <code>devel-some-feature-mariadb</code>, etc.), URL: https://devel-some-feature.aws.lib.msu.edu</p> </li> <li> <p>branch: <code>nothing-special</code> stack prefix: None, no environment created URL: None, no environment created</p> </li> </ul> <p>Note</p> <p>The <code>traefik</code> stack is shared by all of the stacks because it controls routing public requests to the relavent stack based on the host name.</p> <p>The <code>devel</code> and <code>review</code> environments will have the following extra characteristics:</p> <ul> <li>An extra job in the pipeline that can be manually run to clean up   the environment when you are done with it</li> <li>Only a subset of the harvest records will be imported</li> </ul>"},{"location":"CICD/#pipeline-stages-jobs","title":"Pipeline Stages &amp; Jobs","text":""},{"location":"CICD/#test","title":"test","text":"<p>branches: <code>main</code>, <code>catalog-preview</code>, <code>devel-</code>*, and <code>review-</code>*</p> <ul> <li>Runs templates included with GitLab CI/CD to scan for secrets used   in committed code</li> <li>Runs <code>shellcheck</code> on all bash scripts in the repository</li> </ul>"},{"location":"CICD/#build","title":"Build","text":"<p>branches: <code>main</code>, <code>catalog-preview</code>, <code>devel-</code>*, and <code>review-</code>*</p> <ul> <li>Builds all the images in this repository, tagging them with <code>latest</code>   only if it is the <code>main</code> branch</li> <li>When building the VuFind image, it will also perform unit testing   of the <code>Catalog</code> module</li> </ul>"},{"location":"CICD/#deploy","title":"Deploy","text":"<p>branches: <code>main</code>, <code>catalog-preview</code>, <code>devel-</code>*, and <code>review-</code>*</p> <ul> <li>Will set the <code>STACK_NAME</code> variable that is used throughout the pipeline,   which is essentially the branch name unless the branch does not start with   <code>devel-</code>, <code>review-</code> or is <code>main</code></li> <li>Will make updates to the docker compose files and copy them to the AWS   servers. The updates include changing the image tag from <code>:latest</code> to   the current commit SHA and modifying services based on the <code>STACK_NAME</code></li> <li>Will call the playbook that creates a DNS record for devel and review   environments if necessary</li> <li>Deploy both the traefik (which handles routing of public traffic to the   different environments hosted on the swarm) and the internal network used   by the MariaDB Galera services within the individual environment</li> <li>Will bootstrap the <code>solr</code> and <code>mariadb</code> stacks if they have not already   been (i.e. this is the first time running this job for this branch)</li> <li>Deploys the <code>catalog</code>, <code>solr</code>, <code>swarm-cleanup</code>, and <code>mariadb</code> stacks.   If this is a devel or review environment, it will import a single MARC   file into the VuFind instance as test data</li> <li>Runs VuFind version upgrades, if applicable</li> <li>If on a the <code>main</code> branch, it will run functional testing with the tests   in the <code>Catalog</code> module</li> <li>If it is a <code>devel-</code> or <code>review-</code> branch, it will populate the environment   with sample data</li> <li>Evaluate the health of the services on all nodes</li> </ul>"},{"location":"CICD/#clean-up","title":"Clean-up","text":"<p>branches: <code>devel-</code>* and <code>review-</code>*</p> <ul> <li>Removes the stacks, their volumes, and runs the playbook to remove the DNS   record created for the environment</li> </ul>"},{"location":"CICD/#release","title":"Release","text":"<p>branches: <code>main</code></p> <ul> <li>Creates a release tag for the current commit</li> <li>Pushes the latest changes to GitHub and publishes the GitHub Pages once   the GitHub Action job should have completed that compiles the docs</li> </ul>"},{"location":"CICD/#variables","title":"Variables","text":"<p>At this time, the following variables need to be defined in the project's CI/CD settings to be available to the pipeline. While it is ok for variables to be marked as <code>masked</code>, they can not be marked as <code>protected</code>; otherwise they will not be available in the <code>devel-</code> and <code>review-</code> pipelines. You may need to define the same variable multiple times, but for each environment, so that each site has different values. For example, your development environments might have different values for <code>FOLIO_URL</code> then the production environment. This is done using the scope setting in the variables menu. And the scope value is the branch name you want to match it to followed by a wildcard (<code>*</code>). For example: <code>devel-mytest*</code>.</p> <ul> <li><code>AUTH_FTP_PASSWORD</code>: Password for <code>AUTH_FTP_USER</code></li> <li><code>AUTH_FTP_USER</code>: Username for the authority MARC file FTP server</li> <li><code>AWS_KEY</code>: The AWS access key to use when provisioning the DNS CNAME records</li> <li><code>AWS_SECRET</code>: The AWS secret for the <code>AWS_KEY</code> uses when provisioning   the DNS CNAME records</li> <li><code>BASICAUTH_FOR_RESOURCES</code>: Bcrypt password hash[^1] for basic authentication   to internal resources such as Solr and the Traefik dashboard</li> <li><code>BROWZINE_LIBRARY</code>: Library ID for BrowZine (LibKey)</li> <li><code>BROWZINE_TOKEN</code>: BrowZine API token (LibKey)</li> <li><code>DEPLOY_HOST_1</code>: Server FQDN for the for first node in the cluster to   deploy to</li> <li><code>DEPLOY_HOST_2</code>: Server FQDN for the for second node in the cluster to   deploy to</li> <li><code>DEPLOY_HOST_3</code>: Server FQDN for the for thirds node in the cluster to   deploy to</li> <li><code>DEPLOY_KEY_FILE</code>: File path containing GitLab read-only deploy key   base64 encoded</li> <li><code>DEPLOY_PRIVATE_KEY</code>: The <code>base64</code> encoded private SSH key to the   deploy server</li> <li><code>EDS_ORG</code>: Organization ID for the EDS API</li> <li><code>EDS_PASS</code>: Password for the <code>EDS_USER</code> username</li> <li><code>EDS_PROFILE</code>: Profile name for EDS</li> <li><code>EDS_USER</code>: Username for the EDS API</li> <li><code>EMAIL</code>: Email address set in VuFind's configs</li> <li><code>FEEDBACK_EMAIL</code>: Email address for sending feedback form submissions to   (internal and external)</li> <li><code>FEEDBACK_PUBLIC_EMAIL</code>: Email address for sending external feedback form   submissions to</li> <li><code>FOLIO_CANCEL_ID</code>: The FOLIO cancellation ID to use when canceling an   order. VuFind uses <code>75187e8d-e25a-47a7-89ad-23ba612338de</code> by default</li> <li><code>FOLIO_PASS</code>: Password for the <code>FOLIO_USER</code> application user used by VuFind</li> <li><code>FOLIO_REC_ID</code>: Record ID in FOLIO to search for to verify the tenant   is available</li> <li><code>FOLIO_TENANT</code>: Tenant ID</li> <li><code>FOLIO_URL</code>: Okapi URL for FOLIO used by VuFind</li> <li><code>FOLIO_USER</code>: Application user used by VuFind for ILS calls</li> <li><code>HLM_FTP_PASSWORD_FILE</code>: File path containing the password for <code>HLM_FTP_USER</code></li> <li><code>HLM_FTP_USER</code>: Username for the EBSCO FTP server</li> <li><code>GITHUB_USER_TOKEN</code>: Token used to publish releases to GitHub repository</li> <li><code>GOAWAY_JWT_PRIVATE_KEY_SEED</code>: Seed for captcha challenge verification; this   should be set to a unique 64 char hex value, which allows captcha   containers to validate challenges across replicas.</li> <li><code>MATOMO_SEARCHBACKEND_DIMENSION</code>: ID for the custom dimension in Matomo to   track the search backend used for the request</li> <li><code>MATOMO_SITE_ID</code>: Matomo site identifier for the website you want the   analytics sent to</li> <li><code>MATOMO_URL</code>: Matomo URL to send the analytics to</li> <li><code>OAI_URL</code>: URL for making OAI calls to FOLIO when harvesting (can include   API Token)</li> <li><code>RECAPTCHA_SECRET_KEY</code>: Secret key for reCaptcha form validation</li> <li><code>RECAPTCHA_SITE_KEY</code>: Site key for reCaptcha form validation</li> <li><code>REGISTRY_ACCESS_TOKEN</code>: Read-only registry access token used by deploy user</li> <li><code>ROUND_ROBIN_DNS</code>: The DNS A-record entry that is a round-robin to all nodes   in the cluster</li> <li><code>RO_CICD_TOKEN</code>: Read-only access token to this repository used to query the   API during the CI</li> <li><code>RW_CICD_TOKEN</code>: Read-Write access token to this repository used to create   release tags</li> <li><code>SESSION_BOT_SALT</code>: Secure random string used in creating persisting session   ids for bots (when bot_agent values are set)</li> <li><code>SIMPLESAMLPHP_ADMIN_PW_FILE</code>: File path containing the password to the admin   interface of SimpleSAMLphp</li> <li><code>SIMPLESAMLPHP_SALT</code>: Random salt for SimpleSAMLphp</li> <li><code>VUFIND_CORE_INSTALLATION</code>: Set to <code>1</code> in the pipelines to install core-vufind   without MSUL Catalog module. Set to <code>0</code> otherwise.</li> </ul>"},{"location":"CICD/#scheduled-pipelines","title":"Scheduled Pipelines","text":"<p>For the Ansible image to build overnight, saving time on regular daily builds, we can set up a scheduled pipeline to run off-hours in GitLab. This is done in the Schedules tab in the CI/CD page of GitLab. You should configure it for the <code>main</code> branch and set it to run at whatever time is convenient for your team.</p> <p>[^1]     There are many ways to generate this password hash, such as online     generators or command line tools (like <code>htpasswd</code> in the <code>apache-utils</code>     package, for example: <code>htpasswd -B -C 10 -n [username]</code>).     For Traefik performance reasons, we recommend you use a bcrypt cost     value of 10.</p>"},{"location":"alphabrowse/","title":"Alphabrowse Internals","text":"<p>To understand this, it is best to read the first author's documentation: The VuFind Browse Handler and then read Alphabetical Heading Browse in VuFind's documentation.</p> <p>One thing that is not mentioned in that VuFind documentation page is the Solr field displayed for autocomplete is in <code>searches.ini</code>, section <code>Autocomplete_Types</code>.</p> <p>Our customizations were to:</p> <ul> <li>Add a new browse type: <code>series</code> (PC-110)</li> <li>Add a new normalizer: <code>TitleNormalizer</code> (PC-362);   it is part of VuFind now (VUFIND-1630)</li> <li>Add alternative titles to title browse (PC-686,   PC-1151); this required   using a custom function for import (<code>BrowseUtilMixin</code>)</li> <li>Improve autocomplete suggestions (PC-1151,   PC-1181)</li> </ul> <p>We use 2 new schema fields for title browse: <code>title_browse</code> and <code>title_browse_sort</code> to respectively display and sort title browse entries. They are both multivalued (as opposed to VuFind's default <code>title_fullStr</code> and <code>title_sort</code>), and the values should match between the 2 (in practice they don't when a filtered title ends up being empty). The values for these fields are created with <code>BrowseUtilMixin</code> (referenced by <code>marc_local.properties</code>), and used when creating the heading databases from <code>index-alphabetic-browse.sh</code>.</p>"},{"location":"backup-and-restore/","title":"Backup and Restore","text":""},{"location":"backup-and-restore/#backup","title":"Backup","text":"<p>Included with this repository is a script and automated job to back up both the MariaDB database, the Solr index, and the alphabetical browse database files to the shared storage location (<code>/mnt/shared</code>).</p> <p>The script is part of the <code>vufind</code> image used by the <code>cron</code> service in the <code>catalog</code> stack. The shared storage is mounted on the <code>solr</code> service so that the cluster has the ability to write its snapshot data directly to it. For more information on how SolrCloud backups work, see the official documentation.</p> <p>The database backup is a dump of all the non-session tables while putting the galera node into a desynchronized state while the backup is running to help ensure the backup is in a more consistent state. In case the galera cluster ever gets into a de-clustered state, this backup script will take a dump from all three of the galera nodes just to be safe.</p> <p>The automated job will run on a nightly basis and keep a rolling rotation of 3 backups of both the database and Solr index.</p> <p>The code for the backup script can be found at: backup.sh The schedule for the automated cron job can be found at: crontab</p>"},{"location":"backup-and-restore/#manual-backup","title":"Manual backup","text":"<p>If you want to manually run the script from the <code>catalog_cron</code> container ( or the <code>catalog_catalog</code> container) to ensure it runs successfully you could run something similar to:</p> <pre><code>./backup.sh --db --solr --verbose\n</code></pre>"},{"location":"backup-and-restore/#restore","title":"Restore","text":"<p>Should the need to restore from one of these backups arise, simply use the provided restore script giving it the path to the compressed backup you want to restore using. You can restore one or more Solr collections at a time as well as the database.</p> <p>Make sure the <code>MARIADB_ROOT_PASSWORD</code> env variable is defined (it will be in <code>catalog-cron</code>).</p> <p>The code for the backup script can be found at: restore.sh</p> <p>Examples for restoring from backups:</p> <pre><code># Restoring the `biblio` index\n./restore.sh --biblio /mnt/shared/backups/solr/biblio/snapshot.123.tar.gz\n\n# Restoring the database\n./restore.sh --db /mnt/shared/backups/db/dbbackup.tar\n\n# Restore the database using the backup from node 2\n./restore.sh --db /mnt/shared/backups/db/dbbackup.tar --node 2\n\n# Restoring the `authority` and `biblio` index\n./restore -b /tmp/biblio.tar.gz -a /tmp/authority.tar.gz -v\n\n# Restoring the alphabrowse database\n./restore.sh --alpha /mnt/shared/backups/alpha/20240116152918.tar\n</code></pre>"},{"location":"bots/","title":"Bots","text":"<p>When bots behave themselves and follow the rules, they are perfectly welcome to crawl our site without it negatively impacting other users. But there are some bots out there that make as many requests as they can without following the configured bot settings. In these cases we want to block them since they can over-utilize resources and slow down the site significantly for other users.</p>"},{"location":"bots/#generalized-bot-protection-service-go-away","title":"Generalized bot protection service: go-away","text":"<p>The catalog comes with a built-in configurable <code>captcha</code> service based on the <code>go-away</code> bot detection tool, which we've modified to add a custom theme to. (original source repo, modified source repo)</p> <p>This captcha service proxies all traffic between Traefik and VuFind containers so that it can identify and provide \"challenges\" for visitors. These challenges are automatic and do not require any user input, but may in some cases show a temporary challenge page for a second or two the first time a user visits.</p> <p>The configuration and rule definition for what traffic is given challenges and what traffic is considered okay versus suspected bots are available in the files:</p> <ul> <li><code>captcha/config.yml</code></li> <li><code>captcha/rules.yml</code></li> </ul> <p>Documentation on how to configure <code>go-away</code> is available on their wiki.</p>"},{"location":"bots/#how-to-identify-and-block-specific-bot-sources","title":"How to identify and block specific bot sources","text":"<ol> <li> <p>To identify that there is a bot \"attack\" happening, you can check the CPU    and memory usage of the catalog container. If you see unusually high CPU load    versus normal traffic, this might be an indicator of bots. Additionally,    if you see a rise in the number of requests to the catalog (such as from the    Apache Requests Graph    on the monitoring site), this can be an indication that bots are crawling    the site aggressively    <pre><code># Checking container stats for CPU usage\ndocker stats $(docker ps -q -f name=catalog-prod-catalog_catalog)\n</code></pre></p> </li> <li> <p>Find a time when you suspect the bots are causing excess load and grab the    Apache access logs for the stack to identify the IP(s) or user agent(s):    <pre><code># Watching current logs in the docker logs volume on a host machine\ntail -f -n10 /var/lib/docker/volumes/catalog-prod_logs/_data/apache/access.log\n# Getting logs for a specific time from an older access log\nzgrep -F \"29/Jul/2025:16:42:\" /var/lib/docker/volumes/catalog-prod_logs/_data/apache/access.log.2.gz\n</code></pre></p> </li> <li> <p>If there is a distinct bot user-agent, just use that for the block. But    if you only have an IP, then determine the best CIDR range to block.    The output from the <code>whois</code> command can help you identify the larger CIDR range    boundaries that you'll want to block (if no CIDR is provided by <code>whois</code> then    you may need to compute this yourself).    <pre><code>whois [BAD-IP]\n\n...\n% Information related to '[START-IP] - [END-IP]'\n...\nnetname:        [BOT NAME]\n...\n</code></pre></p> </li> <li> <p>Be cautious to not block what might be real users; though bots often fake their    user agents to appear like regular browser, sometimes real users might have    an unusual user agent too. Look for wider patterns to help identify bots.    But once you have a CIDR range or user agent you know you want blocked, go    and edit the <code>policy.yml</code> file for the <code>captcha</code> service. You could create    a new rule if needed, but existing rules are likely sufficient. Consider adding    IP ranges to the <code>malicious</code> network list and user agents to the    <code>undesired-crawlers</code> rule.</p> </li> <li> <p>Deploying the change will apply the bot change to the specific environment    only (e.g.  <code>devel-mysite</code> or <code>catalog-preview</code>); you will need to merge into    <code>main</code> and deploy to production for the change to take effect.</p> </li> </ol>"},{"location":"bots/#emergency-live-changes","title":"Emergency Live Changes","text":"<p>If you need to make an emergency change, you can do so live, but do so with great caution. Making a live change will be overwritten next time that environment is deployed. To make the change, either scale the <code>captcha</code> service down to a single container, or you can make the change individually in all 3 <code>captcha</code> containers.</p> <p>For example, to make live changes to <code>catalog-prod</code> deployment:</p> <pre><code># Scale down to 1 container\ndocker service scale catalog-prod-catalog-captcha=1\n# Connect to that captcha container\npc-connect catalog-prod-catalog-captcha -c bash\n\n# Inside the container, edit the policy.yml file as desired\nvim /policy.yml\n# Once saved, send a SIGHUP to the go-away service to reload configs\nkill -HUP 1\n</code></pre> <p>The live change should now be applied to that one running container.</p>"},{"location":"communication-processes/","title":"Communication Processes","text":"<p>This document contains who to contact and when in various circumstances.</p>"},{"location":"communication-processes/#minor-vufind-version-upgrades","title":"Minor VuFind Version Upgrades","text":""},{"location":"communication-processes/#what-qualifies","title":"What Qualifies","text":"<ul> <li>Any VuFind minor release upgrade. Which would include increments   to the 2nd or 3rd number of the version (1.2 to 1.3 or 1.2.1 to 1.2.2).</li> </ul>"},{"location":"communication-processes/#what-information-to-include-in-communication","title":"What Information to Include in Communication","text":"<ul> <li>A link to the release notes</li> <li>Timeframe when the upgrade will occur</li> </ul>"},{"location":"communication-processes/#communication-method","title":"Communication Method","text":"Who to Contact When to Contact LIB.DL.libstaff@msu.edu Within the week prior to the upgrade"},{"location":"communication-processes/#major-vufind-version-upgrades","title":"Major VuFind Version Upgrades","text":""},{"location":"communication-processes/#what-qualifies_1","title":"What Qualifies","text":"<ul> <li>Any VuFind major release upgrade. Which would include increments   to the 1st number of the version (1.2 to 2.1 or 1.9.1 to 2.0.0).</li> </ul>"},{"location":"communication-processes/#what-information-to-include-in-communication_1","title":"What Information to Include in Communication","text":"<ul> <li>A link to the release notes</li> <li>Timeframe when the upgrade will occur</li> <li>Highlights of any key changes in the upgrade that impact our institution</li> </ul>"},{"location":"communication-processes/#communication-method_1","title":"Communication Method","text":"Who to Contact When to Contact LIB.DL.CDAWG@msu.edu 1 month prior to the upgrade Library Staff Newsletter Newsletter edition prior to the upgrade LIB.DL.libstaff@msu.edu Within the week prior to the upgrade"},{"location":"communication-processes/#vufind-outages","title":"VuFind Outages","text":""},{"location":"communication-processes/#what-qualifies_2","title":"What Qualifies","text":"<ul> <li>Any time the production site is down for more than 15 minutes</li> </ul>"},{"location":"communication-processes/#what-information-to-include-in-communication_2","title":"What Information to Include in Communication","text":"<ul> <li>A general estimate of when service will be restored subject   to change and future updates</li> <li>Any information on the cause of the outage, if known</li> </ul>"},{"location":"communication-processes/#communication-method_2","title":"Communication Method","text":"Who to Contact When to Contact LIB.DL.libstaff@msu.edu When the outage has lasted longer than 15 minutes LIB.DL.libstaff@msu.edu When service is restored"},{"location":"communication-processes/#minor-feature-updates","title":"Minor Feature Updates","text":""},{"location":"communication-processes/#what-qualifies_3","title":"What Qualifies","text":"<ul> <li>Changes to data displayed on the page (such as fields shown on the   record page)</li> <li>Small changes to the theme (including accessibility improvements)</li> <li>Bug fixes to existing features</li> </ul>"},{"location":"communication-processes/#what-information-to-include-in-communication_3","title":"What Information to Include in Communication","text":"<ul> <li>What change is being made, with links and screenshots if applicable</li> </ul>"},{"location":"communication-processes/#communication-method_3","title":"Communication Method","text":"Who to Contact When to Contact LIB.DL.CDAWG@msu.edu PubCat sprint review"},{"location":"communication-processes/#major-feature-updates","title":"Major Feature Updates","text":""},{"location":"communication-processes/#what-qualifies_4","title":"What Qualifies","text":"<ul> <li>Adding/changing/removing a UI feature that could affect training videos</li> </ul>"},{"location":"communication-processes/#what-information-to-include-in-communication_4","title":"What Information to Include in Communication","text":"<ul> <li>What change is being made, with links and screenshots if applicable</li> <li>Where they can preview the change</li> <li>When the change will be on the production site</li> </ul>"},{"location":"communication-processes/#communication-method_4","title":"Communication Method","text":"Who to Contact When to Contact LIB.DL.CDAWG@msu.edu PubCat sprint review Library Staff Newsletter Newsletter edition after it is on the preview site"},{"location":"core-vufind/","title":"Using Core VuFind Only","text":"<p>When developing a feature for core VuFind you do not want MSUL catalog code. To more easily enable this, we added the ability to create a new environment not including any of our local customizations.</p> <p>To do so, when running the pipelines on your environment (on <code>devel-*</code> environments only), pass the variable <code>VUFIND_CORE_INSTALLATION</code> with the value of <code>1</code> which will adapt the setup of the environment to exclude MSUL Catalog module.</p> <p>In GitLab, on the <code>Pipelines</code> page, click the button <code>Run pipeline</code>, choose your <code>devel-*</code> branch, Use <code>VUFIND_CORE_INSTALLATION</code> as Input variable key and type <code>1</code> as Input variable value.</p>"},{"location":"documentation/","title":"Documentation","text":""},{"location":"documentation/#github-pages","title":"GitHub Pages","text":"<p>This site is hosted on GitHub Pages. It is build and deployed via GitHub Actions.</p>"},{"location":"documentation/#troubleshooting","title":"Troubleshooting","text":"<p>If there are issues with the page displaying there are two things you can try.</p> <ol> <li> <p>Change the branch in the Pages Settings    to something other than <code>gh-pages</code>, save it, then wait a few minutes, then    change it back to <code>gh-pages</code> and make sure the path is <code>/ (root)</code> and save    it again. It will take a few minutes for the GitHub Action to run to    redeploy the update before you can check again.</p> </li> <li> <p>If that does not work, verify the output of the GitHub Action job log to make    sure there were no errors building the documentation site. Find the most    recent pipeline    then click into it, and then into the <code>deploy</code> job. You can expand each    section to see messages. The one most likely to cause issues would be the step    called <code>Run mkdocs gh-deploy --force</code>. There could be errors listed like    syntax errors in the <code>.md</code> files, or possibly even just re-running    the job could resolve the issue (maybe in combination with repeating    step 1 afterwards).</p> </li> </ol>"},{"location":"documentation/#building-locally-for-testing","title":"Building locally for testing","text":"<p>It is possible to build the GitHub pages documentation site locally on your machine to test it before you commit and deploy it.</p> <pre><code>pip install mkdocs-material mkdocs-autorefs mkdocs-material-extensions mkdocstrings\ncd ${CATALOG_REPO_DIR}\npython3 -m mkdocs serve\n# OR\npython3 -m mkdocs serve -a localhost:9000\n</code></pre> <p>You should now be able to open your browser to the link in the <code>serve</code> output to verify. Additionally, the output of the serve command would display any errors with building the site.</p> <p>As long as the <code>serve</code> command is running, it will auto-detect changes to the files so you can continue to modify them and see the updates in your browser.</p>"},{"location":"documentation/#running-checks-on-markdown-files","title":"Running checks on Markdown files","text":"<pre><code>cd ${CATALOG_REPO_DIR}\n# Lint checks\ndocker run --rm -it \\\n  -v $PWD:/code \\\n  registry.gitlab.com/pipeline-components/markdownlint-cli2:latest \\\n  markdownlint-cli2 \"**/**.md\"\n\n# Spell checks\ndocker run --rm -it \\\n  -v $PWD:/code \\\n  registry.gitlab.com/pipeline-components/markdown-spellcheck:latest \\\n  mdspell --report '**/*.md' --ignore-numbers --ignore-acronyms\n</code></pre>"},{"location":"first-time-setup/","title":"First Time Setup","text":""},{"location":"first-time-setup/#building-the-vufind-image","title":"Building the VuFind image","text":"<p>Before bringing up the application stack, you need to build the custom VuFind images. There are helper scripts in this repository to build one for each service, such as VuFind and Solr.</p> <p>The CICD will do this for you automatically, but if you want to do it manually you can still use the scripts.</p> <p>The more complex one, VuFind, has its own script which includes the documentation on which environment variables it expects to be defined when calling the script. For example:</p> <pre><code># Build vufind:new using vufind:latest's build cache with the 9.0.1\n# VuFind version\n# NOTE: this is missing many other required environment variables to\n# be a running VuFind environment and is just meant to be an example\n# to get you started!\nLATEST=vufind:latest CURR=vufind:new VUFIND_VERSION=9.0.1 cicd/build-vufind\n</code></pre> <p>For the rest of the components, they share a single script to build their images and can be run like this:</p> <pre><code># Build the Solr image as vufind:new using solr:latst's build cache using data from\n# VuFind 9.0.1 (in this case the Solr schema and solrconfig)\nLATEST=solr:latest CURR=solr:new COMPONENT=solr VUFIND_VERSION=9.0.1 cicd/build-general\n\n# Now do the rest!\nLATEST=zk:latest CURR=zk:new COMPONENT=zk VUFIND_VERSION=9.0.1 cicd/build-general\nLATEST=db:latest CURR=db:new COMPONENT=db VUFIND_VERSION=9.0.1 cicd/build-general\nLATEST=ansible:latest CURR=ansible:new COMPONENT=ansible VUFIND_VERSION=9.0.1 cicd/build-general\nLATEST=monitoring:latest CURR=monitoring:new COMPONENT=monitoring VUFIND_VERSION=9.0.1 cicd/build-general\nLATEST=legacylinks:latest CURR=legacylinks:new COMPONENT=legacylinks VUFIND_VERSION=9.0.1 cicd/build-general\n</code></pre>"},{"location":"first-time-setup/#to-start-the-application-stack","title":"To start the application stack","text":"<p>During the first time you are bringing up the stack, you will need to run these first to bootstrap Solr and MariaDB:</p> <pre><code>docker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.solr-cloud-bootstrap.yml) solr\ndocker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.mariadb-cloud-bootstrap.yml) mariadb\n</code></pre> <p>Subsequently, you will run these commands (during the initial deploy and whenever you need to deploy updates):</p> <pre><code># Public network\ndocker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.public.yml) public\n\n# Traefik stack to handle networking\ndocker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.traefik.yml) traefik\n\n# Internal network for galera cluster\ndocker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.internal.yml) internal\n\n# The rest of the MariaDB galera cluster\ndocker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.mariadb-cloud.yml) mariadb\n\n# The rest of the Solr cloud stack\ndocker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.solr-cloud.yml) solr\n\n# The VuFind stack\ndocker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.catalog.yml) catalog\n\n# Deploy the swarm cleanup stack\ndocker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.swarm-cleanup.yml) swarm-cleanup\n\n# Deploy the monitoring stack\ndocker stack deploy -c &lt;(source .env; envsubst &lt;docker-compose.monitoring.yml) monitoring\n</code></pre>"},{"location":"first-time-setup/#creating-a-folio-user","title":"Creating a FOLIO user","text":"<p>In order for VuFind to connect to FOLIO to make API calls, it requires a generic user to be created, called <code>vufind</code>.</p> <p>The users credentials are provided as build arguments to the VuFind image: <code>FOLIO_USER</code> and <code>FOLIO_PASS</code>.</p> <p>The <code>vufind</code> application user (set in <code>local/confing/vufind/folio.ini</code>) requires the following permissions within FOLIO. They need to be created as a permission set with the FOLIO API, with a <code>POST</code> request to <code>/perms/permissions</code>.</p> <ul> <li><code>inventory.instances.item.get</code></li> <li><code>inventory.items-by-holdings-id.collection.get</code> (Required as of Ramsons for   <code>/inventory/items-by-holdings-id</code>)</li> <li><code>inventory.items.collection.get</code></li> <li><code>inventory.items.item.get</code></li> <li><code>inventory-storage.bound-with-parts.collection.get</code></li> <li><code>inventory-storage.holdings.collection.get</code></li> <li><code>inventory-storage.holdings.item.get</code></li> <li><code>inventory-storage.instances.collection.get</code></li> <li><code>inventory-storage.items.collection.get</code></li> <li><code>inventory-storage.items.item.get</code></li> <li><code>inventory-storage.locations.collection.get</code></li> <li><code>inventory-storage.locations.item.get</code></li> <li><code>inventory-storage.loan-types.collection.get</code> (For MSUL customization   to display loan type)</li> <li><code>inventory-storage.service-points.collection.get</code></li> <li><code>circulation.loans.collection.get</code></li> <li><code>circulation.requests.allowed-service-points.get</code></li> <li><code>circulation.requests.item.post</code></li> <li><code>circulation.requests.item.get</code></li> <li><code>circulation.requests.item.put</code></li> <li><code>circulation.renew-by-id.post</code></li> <li><code>circulation-storage.requests.collection.get</code></li> <li><code>circulation-storage.request-preferences.collection.get</code></li> <li><code>users.collection.get</code></li> <li><code>ui-requests.view</code></li> <li><code>accounts.collection.get</code></li> <li><code>course-reserves-storage.courselistings.collection.get</code></li> <li><code>course-reserves-storage.courselistings.courses.collection.get</code></li> <li><code>course-reserves-storage.courselistings.instructors.collection.get</code></li> <li><code>course-reserves-storage.courses.collection.get</code></li> <li><code>course-reserves-storage.departments.collection.get</code></li> <li><code>course-reserves-storage.reserves.collection.get</code></li> <li><code>course-reserves-storage.terms.collection.get</code></li> <li><code>oai-pmh.records.collection.get</code></li> <li><code>kb-ebsco.packages.collection.get</code> (For MSUL customization to add license   agreement information to record pages)</li> <li><code>proxiesfor.collection.get</code></li> </ul>"},{"location":"first-time-setup/#for-gitlab-users","title":"For GitLab users","text":""},{"location":"first-time-setup/#creating-a-cicd-token","title":"Creating a CI/CD Token","text":"<p>Create a new access token that has <code>read_registry</code> privileges to the repository and create a new CI/CD variable with the resulting key value (<code>REGISTRY_ACCESS_TOKEN</code>).</p>"},{"location":"first-time-setup/#create-cicd-variables","title":"Create CI/CD Variables","text":"<p>There are a number of variables that are required for the CI/CD pipeline to run. Refer to the CI/CD variables section for details.</p>"},{"location":"first-time-setup/#for-local-development","title":"For local development","text":"<p>To make changes to the theme, custom module, or files stored within <code>local</code> you can either modify them directly inside the containers, or you can mount the shared storage and make changes there. Changes to the live storage are symbolically linked to the containers and will appear real time in the environment -- very handy for theme development! This is only available for development environment starting with devel-*.</p> <p>Within the shared storage there will be a subdirectory for each branch name. This documentation assumes that the share has been set up and configured already on the hosts. The subdirectory will contain a clone of this repository which can be easily used to track changes between subsequent deploys to the same branch.</p> <p>Note that subsequent deploys only do a <code>git fetch</code> to avoid overwriting local changes. You are responsible for doing a <code>git pull</code> to apply new changes.</p>"},{"location":"harvesting-and-importing/","title":"Harvesting &amp; Importing","text":"<p>Note</p> <p>All of these commands should be run within the <code>catalog</code> or <code>cron</code> Docker container</p>"},{"location":"harvesting-and-importing/#script-summary","title":"Script Summary","text":"<ul> <li>pc-import-folio:   Used to harvest and import FOLIO MARC data into the <code>biblio</code> collection   of Solr.</li> <li>pc-import-hlm:   Used to harvest and import EBSCO MARC data into the <code>biblio</code> collection   of Solr from the FTP location given access to by EBSCO. The records   contain the HLM dataset that is missing from FOLIO's database.</li> <li>pc-import-authority:   Used to harvest and import MARC data from Backstage into the <code>authority</code>   collection in Solr from the FTP location provided by Backstage.</li> <li>cron-reserves.sh:</li> <li>pc-full-import:   Wrapper script for the <code>pc-import-folio</code> and <code>pc-import-hlm</code> scripts to   perform a full import of data.</li> </ul>"},{"location":"harvesting-and-importing/#hlm-data","title":"HLM Data","text":"<p>One of our sources is HLM (Holdings Link Management) which is files from EBSCO's FTP server with electronic resources our library has access to which are not in FOLIO. To automate the retrieval and import of these files we have a wrapper script, <code>pc-import-hlm</code>.</p> <p>Since occasionally EBSCO sends us full sets of data, we want to be able to exclude all previous sets from them, so to do this we have an ignore files <code>/mnt/shared/oai/ignore_patterns.txt</code> with will check to see if the file name contains any of those substrings and ignore them if they match.</p> <p>Also, we will only harvest files that match the pattern <code>*.m*c</code> and <code>*.zip</code>. <code>*.m*c</code> files will be imported with the exception of <code>-del*.m*c</code> (case insensitive) or <code>.delete</code> files which will instead be tagged as deletion files.</p> <p>If you simply want to see what files are on the FTP server currently, you can run our helper script, <code>pc-list-hlm-remote</code>, to list all the files. If you want to download a specific file, run <code>pc-get-hlm-remote [NAME OF FILE]</code>.</p>"},{"location":"harvesting-and-importing/#full-data-harvests","title":"Full Data Harvests","text":"<p>This section describes the steps needed to re-harvest all the data from each source.</p>"},{"location":"harvesting-and-importing/#folio","title":"FOLIO","text":"<ol> <li> <p>Ensure that your OAI settings on the FOLIO tenant are what you want    them to be for this particular harvest. For example, if you wish to    include storage and inventory records (i.e. the records without a    MARC source) then you will need to modify the \"Record Source\" field    in the OAI Settings in FOLIO.</p> </li> <li> <p>Next you will need to clear out the contents of the <code>harvest_folio</code>    directory before the next cron job will run. Assuming you want to    preserve the last harvest for the time being, you can simply move    those directories somewhere else and rename them. Below is an example,    but certainly not the only option. The only goal is that the    <code>harvest_folio</code> directory has no files in it, but can have the <code>log</code>    and <code>processed</code> directories within it as long as they are empty    (they technically can have files in them, you just will not want    them to have files since they will get mixed in with your new harvest).    <pre><code>cd /mnt/shared/oai/[STACK_NAME]/harvest_folio/\nmv processed processed_old\nmv log log_old\nmv last_state.txt last_state.txt.old\nmv harvest.log harvest.log.old\n</code></pre></p> </li> <li> <p>Monitor progress after it starts via the cron job in the monitoring app    or in the log file on the container or volume (<code>/mnt/logs/harvests/</code>).</p> </li> </ol>"},{"location":"harvesting-and-importing/#hlm","title":"HLM","text":"<p>This can just be done with the script's <code>--full</code> <code>--harvest</code> flags in a one-off run, but if you prefer to have it run via the cron job use it's normal flags, here are the steps you would need to do in order to prepare the environment.</p> <ol> <li> <p>Remove all files from the <code>/mnt/shared/hlm/[STACK_NAME]/harvest_hlm/</code>    directory. You can also just move them somewhere else if you want to preserve a copy of them.    <pre><code>cd /mnt/shared/hlm/[STACK_NAME]/harvest_hlm/\nmv processed processed_old\nmv log log_old\n</code></pre></p> </li> <li> <p>Monitor progress after it starts via the cron job in the monitoring app or    in the log file on the container or volume (<code>/mnt/logs/harvests/</code>).</p> </li> </ol>"},{"location":"harvesting-and-importing/#backstage-authority-records","title":"Backstage (Authority records)","text":"<p>This can just be done with the script's <code>--full</code> <code>--harvest</code> flags in a one-off run, but if you prefer to have it run via the cron job use it's normal flags, here are the steps you would need to do in order to prepare the environment.</p> <ol> <li> <p>Remove all files from the <code>/mnt/shared/authority/$STACK_NAME/harvest_authority/</code>    directory.    You can also just move them somewhere else if you want to preserve a copy    of them.    <pre><code>tar -czf archive_[SOME_DATE].tar.gz /mnt/shared/authority/$STACK_NAME/harvest_authority/\nfind /mnt/shared/authority/$STACK_NAME/harvest_authority/ -type f -delete\n</code></pre></p> </li> <li> <p>Monitor progress after it starts via the cron job in the monitoring app or in the log file on the container or volume (<code>/mnt/logs/harvests/</code>).</p> </li> </ol>"},{"location":"harvesting-and-importing/#full-data-imports","title":"Full Data Imports","text":""},{"location":"harvesting-and-importing/#biblio-index","title":"<code>biblio</code> Index","text":"<p>Helper script for full import</p> <p>There is now a helper script to run all of the below steps in the proper order to do a full re-import of data in the <code>biblio</code> index. See the full documentation for the pc-full-import and the below for a simple example.</p> <pre><code>sudo screen\n# Prompting for confirmation\npc-full-import catalog-prod --debug 2&gt;&amp;1 | tee /mnt/shared/logs/catalog-prod-import_$(date -I).log\n# Bypassing user confirmation and notifying pubcat on completion\npc-full-import catalog-prod --email LIB.DL.pubcat@msu.edu --yes --debug 2&gt;&amp;1 | tee /mnt/shared/logs/catalog-prod-import_$(date -I).log\n</code></pre> <p>Should you choose to do the steps manually, this section will describe the process needed to run a full re-import of the data since that is frequently required to update the Solr index with new field updates. If other tasks are required (such as full harvests or incremental) refer to the <code>--help</code> flags on the appropriate script.</p> <p>Full imports for the <code>biblio</code> collection can be done</p> <ul> <li>directly in the <code>cron</code> container for prod/beta/preview,</li> <li>in the <code>catalog</code> container for dev environments,</li> <li>using the <code>biblio</code> collection alias in the <code>build</code> container, to avoid serving incomplete collections in prod (see How to Use the Collection Aliases to Rebuild and Swap below).</li> </ul>"},{"location":"harvesting-and-importing/#importing-folio-records-using-the-cron-container","title":"Importing FOLIO records using the cron container","text":"<p>Connect to one of the catalog server nodes and move the following files up a level out of the processed directory in the shared storage. This will allow them to be picked up by the next cron job and re-started automatically should the container get stopped due to deployments. Progress can be monitored by checking then number of files remaining in the directory and the log file in <code>/mnt/logs/harvests/folio_latest.log</code>.</p> <pre><code># Can be done inside or outside the container\nmv /mnt/shared/oai/[STACK_NAME]/harvest_folio/processed/* /mnt/shared/oai/[STACK_NAME]/harvest_folio/\n</code></pre>"},{"location":"harvesting-and-importing/#importing-folio-records-in-dev-environments","title":"Importing FOLIO records in dev environments","text":"<p>This will import the tests records. In the <code>catalog</code> container:</p> <pre><code>./pc-import-folio -c /mnt/shared/oai/devel-batch -l 1 -b -v -r\n</code></pre>"},{"location":"harvesting-and-importing/#importing-hlm-records-using-the-cron-container","title":"Importing HLM records using the cron container","text":"<p>Assuming HLM records also need to be updated in the <code>biblio</code> index as well, you will need to copy those files from the shared directory into the container prior to starting the script. Then start a <code>screen</code> session and connect to the container again and run the command to import the files. Note that this will get terminated and not be recoverable if the container stops due to a deploy like the previous command was. Process can be monitored by seeing the remaining files in the <code>/usr/local/harvest/hlm/</code> directory and by re-attaching to the <code>screen</code> (by using <code>screen -r</code>) to see if the command has completed.</p> <pre><code># Done inside the catalog_cron container\ncp /mnt/shared/hlm/[STACK_NAME]/current/* /usr/local/vufind/local/harvest/hlm/\n\n# You will want to kick off this command in a screen session,\n# since it can take many hours to run\n/usr/local/bin/pc-import-hlm -i -v\n</code></pre>"},{"location":"harvesting-and-importing/#how-to-use-the-collection-aliases-to-rebuild-and-swap","title":"How to Use the Collection Aliases to Rebuild and Swap","text":"<p>As mentioned in the Solr documentation, <code>biblio</code> uses aliases to manage directing VuFind to the collection in Solr that have the \"live\" biblio data that should be used for searching: <code>biblio1</code> or <code>biblio2</code>. This means we will on occasion need to swap them. This occasion being when we rebuild the index, such as when we're adding new data fields or doing a VuFind version upgrade (...which typically add new data fields).</p> <ul> <li> <p>Start the manual task \"Deploy VuFind Build Env\" in GitLab. It will update   the <code>catalog_build</code> container. This is not done automatically so that other   updates to the main branch can be deployed while a full import is running.</p> </li> <li> <p>Identify what collection each alias is pointing to currently   (i.e. is <code>biblio</code> pointing to <code>biblio1</code> or <code>biblio2</code>) and confirm   the other collection is what <code>biblio-build</code> is pointing to.   To get the list of aliases, from a container:</p> </li> </ul> <pre><code>curl -s \"http://solr:8983/solr/admin/collections?action=LISTALIASES\" | grep biblio\n</code></pre> <ul> <li>Rebuild the index on <code>biblio-build</code> using the <code>catalog_build</code> container.   This has everything that the <code>catalog_cron</code> containers have access to, but   do not run <code>cron</code> jobs since rebuilds do not happen at regular or frequent   intervals. In fact, all this container does is sleep! It is recommended to   run these commands in a <code>screen</code>.</li> </ul> <p>Warning</p> <p>If you run a deploy pipeline while this is running, you will not want to run the manual job that deploys the updates to the build container (since not all the import scripts are configured to resume where they left off yet).</p> <pre><code># On Host\nscreen\ndocker exec -it $(docker ps -q -f name=catalog-prod-catalog_build) bash\n\n# Inside container\nrm local/harvest/folio/processed/*\ncp /mnt/shared/oai/${STACK_NAME}/harvest_folio/processed/* local/harvest/folio/\n/usr/local/bin/pc-import-folio --verbose --reset-solr --collection biblio-build --batch-import | tee /mnt/shared/logs/folio_import_${STACK_NAME}_$(date -I).log\n[Ctrl-a d]\n\n# On Host\nscreen\ndocker exec -it $(docker ps -q -f name=catalog-prod-catalog_build) bash\n\n# Inside container\ncp /mnt/shared/hlm/${STACK_NAME}/current/* local/harvest/hlm/\n/usr/local/bin/pc-import-hlm --import --verbose | tee /mnt/shared/logs/hlm_import_${STACK_NAME}_$(date -I).log\n[Ctrl-a d]\n</code></pre> <ul> <li>Verify the counts are what you expect on the <code>biblio-build</code> collection   using the following command</li> </ul> <pre><code>curl 'http://solr:8983/solr/admin/metrics?nodes=solr1:8983_solr,solr2:8983_solr,solr3:8983_solr&amp;prefix=SEARCHER.searcher.numDocs,SEARCHER.searcher.deletedDocs&amp;wt=json'\n</code></pre> <ul> <li>Build the spellchecking indices</li> </ul> <p>Building these indices is only necessary for a full import.</p> <pre><code>curl 'http://solr1:8983/solr/biblio-build/select?q=*:*&amp;spellcheck=true&amp;spellcheck.build=true' &amp;\ncurl 'http://solr2:8983/solr/biblio-build/select?q=*:*&amp;spellcheck=true&amp;spellcheck.build=true' &amp;\ncurl 'http://solr3:8983/solr/biblio-build/select?q=*:*&amp;spellcheck.true&amp;spellcheck.build=true' &amp;\nwait\ncurl 'http://solr1:8983/solr/biblio-build/select?q=*:*&amp;spellcheck.dictionary=basicSpell&amp;spellcheck=true&amp;spellcheck.build=true' &amp;\ncurl 'http://solr2:8983/solr/biblio-build/select?q=*:*&amp;spellcheck.dictionary=basicSpell&amp;spellcheck=true&amp;spellcheck.build=true' &amp;\ncurl 'http://solr3:8983/solr/biblio-build/select?q=*:*&amp;spellcheck.dictionary=basicSpell&amp;spellcheck=true&amp;spellcheck.build=true' &amp;\nwait\n</code></pre> <p><code>/bitnami/solr/server/solr/biblioN/spellShingle</code> and <code>/bitnami/solr/server/solr/biblioN/spellchecker</code> should have a significant size afterward in the solr container (replace <code>biblioN</code> by the <code>biblio-build</code> collection)</p> <ul> <li>Your Solr instance will likely require more memory than it typically needs   to do the collection alias swap. Be sure to increase and deploy the stack   with additional <code>SOLR_JAVA_MEM</code> as required to  ensure no downtime during   this step. Currently, 6G (which we use in prod) is enough for the swap.   Alternatively (for beta and preview), let it crash after these commands   and restart the pipeline to help Solr cloud fix itself.</li> </ul> <pre><code># Open the solr-cloud compose file for your environment\nvim docker-compose.solr-cloud.yml\n\n# Modify the memory line to:\nSOLR_JAVA_MEM: -Xms8192m -Xmx8192m\n\n# Now on the host, run the deploy helper script\nsudo pc-deploy [ENV_NAME] solr-cloud\n</code></pre> <ul> <li>Once you are confident in the new data, you are ready to do the swap!   BE SURE TO SWAP THE NAME AND COLLECTION IN THE BELOW COMMAND EXAMPLE</li> </ul> <p>Warning</p> <pre><code># Command to check the aliases (repeated from above)\ncurl -s \"http://solr:8983/solr/admin/collections?action=LISTALIASES\" | grep biblio\n</code></pre> <pre><code># This EXAMPLE sets biblio-build to biblio2, and biblio to biblio1\n# biblio-build =&gt; biblio2\n# biblio       =&gt; biblio1\ncurl 'http://solr:8983/solr/admin/collections?action=CREATEALIAS&amp;name=biblio-build&amp;collections=biblio2'\ncurl 'http://solr:8983/solr/admin/collections?action=CREATEALIAS&amp;name=biblio&amp;collections=biblio1'\n</code></pre> <pre><code># This EXAMPLE sets biblio-build to biblio1, and biblio to biblio2\n# biblio-build =&gt; biblio1\n# biblio       =&gt; biblio2\ncurl 'http://solr:8983/solr/admin/collections?action=CREATEALIAS&amp;name=biblio-build&amp;collections=biblio1'\ncurl 'http://solr:8983/solr/admin/collections?action=CREATEALIAS&amp;name=biblio&amp;collections=biblio2'\n</code></pre> <ul> <li> <p>If needed, back-date the timestamp on your <code>last_harvest.txt</code> re-harvest   some of the OAI changes since you started the import</p> </li> <li> <p>Clear out the collection that <code>biblio-build</code> is pointing to, to avoid   having two large indexing stored for a long period of time (only   after you are confident in the new index's data)</p> </li> </ul> <pre><code>/usr/local/bin/pc-import-folio --verbose --reset-solr --collection biblio-build\n</code></pre> <ul> <li> <p>If <code>SOLR_JAVA_MEM</code> was increased, lower it to its previous amount.</p> </li> <li> <p>Kick off a manual alpha browse re-index if you don't want it to be outdated   until the next scheduled run.</p> </li> </ul> <pre><code># Run this on all of the host's\ndocker exec -it \\\n\u00a0 $(docker ps -q -f name=${STACK_NAME}-solr_cron) \\\n\u00a0 /alpha-browse.sh -v -f\n</code></pre>"},{"location":"harvesting-and-importing/#authority-index","title":"<code>authority</code> Index","text":"<p>Similar to the process for HLM records, copy the files from the shared directory into the containers import location prior to starting the script. Then start a <code>screen</code> session and connect to the container again and run the command to import the files. Process can be monitored by seeing the remaining files in the <code>/usr/local/harvest/authority</code> directory and by re-attaching to the <code>screen</code> (by using <code>screen -r</code>) to see if the command has completed.</p> <pre><code>mv /mnt/shared/authority/$STACK_NAME/harvest_authority/processed/*.[0-9][0-9][0-9].xml /mnt/shared/authority/$STACK_NAME/harvest_authority\n\n# You can either run the command manually in a screen, or let the cron pick it up\n/usr/local/bin/pc-import-authority -i -B\n</code></pre>"},{"location":"harvesting-and-importing/#reserves-index","title":"<code>reserves</code> Index","text":"<p>The course reserves data is refreshed by a Cron job on a nightly basis, so likely you will not need to run this manually if you can just wait for the regular run. But if needed, here is the command to run it off-schedule.</p> <pre><code># Done inside the container (ideally within a screen since it will\n# take hours to run)\nphp /usr/local/vufind/util/index_reserves.php\n</code></pre> <p>Alternatively, you can also modify the cron entry (or add a temporary additional cron entry) in the cron container for the <code>cron-reserves.sh</code> command to run at an earlier time. The benefit of this would be it would save logs to <code>/mnt/logs/vufind/reserves_latest.log</code> and track them in the Monitoring site.</p>"},{"location":"harvesting-and-importing/#adding-generated-call-numbers","title":"Adding generated call numbers","text":"<p>This should be done after each full import (FOLIO + HLM) when data was reset, in the <code>solr_solr</code> container:</p> <pre><code>python3 ./add_generated_call_numbers.py\n</code></pre> <p>Partial call numbers are added to <code>callnumber-label</code> for records that didn't have any when the <code>call_numbers.csv</code> file was generated.</p> <p>Note that the call numbers in <code>/mnt/shared/call-numbers/call_numbers.csv</code> are meant for beta/preview/prod. There is another file at <code>/mnt/shared/call-numbers/test_call_numbers.csv</code> that can be used for testing in dev.</p>"},{"location":"harvesting-and-importing/#ignoring-certain-hlm-files","title":"Ignoring certain HLM files","text":"<p>If your EBSCO FTP server is set up in a way where it contains all of the sets ever generated for you, then you'll likely want a way to have the <code>pc-import-hlm</code> script ignore the past sets assuming you get new full sets periodically. This can be done by adding a new substring pattern to ignore to the top level of the <code>hlm</code> directory in the shared storage (<code>/mnt/shared/hlm/ignore_patterns.txt</code>). This file is used automatically and created on the cron containers startup if it doesn't exist. You can override the file path by using the <code>-p|--ignore-file</code> flag.</p>"},{"location":"harvesting-and-importing/#using-vufind-utilities","title":"Using VuFind Utilities","text":"<p>The preferred method is to use the included wrapper script with this repository. The pc-import-folio script can run either, or both, the harvest and import of data from FOLIO to VuFind. Use the <code>--help</code> flag to get information on how to run that script.</p> <p>But should you choose to run the commands included directly with VuFind, below is documentation on how to do that.</p>"},{"location":"harvesting-and-importing/#harvesting-from-folio","title":"Harvesting from Folio","text":"<pre><code>cd /usr/local/vufind/harvest\nphp harvest_oai.php\n\n## This step is optional, it will combine the xml files into a single file\n## to improve the speed of the next import step.\nfind *.xml | xargs xml_grep --wrap collection --cond \"marc:record\" &gt; combined.xml\nmkdir unmerged\nmv *oai*.xml unmerged/\n</code></pre>"},{"location":"harvesting-and-importing/#importing-into-vufind","title":"Importing into VuFind","text":"<pre><code>cd /usr/local/vufind/harvest\n./batch-import-marc.sh folio\n</code></pre>"},{"location":"helper-scripts/","title":"Helper Scripts","text":"<p>As part of our infrastructure repository (soon to be open source), we have a set of helper scripts to help with common tasks that have long or hard to remember commands. The following documentation serves as a quick reference to know which scripts are available.</p> <p>Each script has its own <code>--help</code> flag to get more detailed information. Each script also offers tab completion for ease-of-use.</p>"},{"location":"helper-scripts/#deploy-helper-pc-deploy","title":"Deploy Helper (pc-deploy)","text":"<p>Deploys stacks for a given environment and docker compose. This is useful because it does the step of sourcing the <code>.env</code> file for the environment directory used and calling <code>envsubstr</code> on the compose file before deploying the stack.</p> <p>Make sure you run it as the <code>deploy</code> user so that the proper Docker container registry credentials are passed. While running as <code>root</code> would also work since the script will detect that and switch users, this documentation will list using the <code>deploy</code> user to help indicate that is the user with the proper credentials to the container registry.</p> <pre><code># Deploy the catalog stack for the catalog-prod environment\nsudo -Hu deploy pc-deploy catalog-prod catalog\n\n# Do a dry-run of the traefik stack, which is a core-stack\nsudo -Hu deploy pc-deploy core-stacks traefik -n\n\n# Deploy the solr bootstrap compose file for the devel-test stack\nsudo -Hu deploy pc-deploy devel-test solr-bootstrap\n# or\nsudo -Hu deploy pc-deploy devel-test docker-compose.solr-bootstrap.yml\n</code></pre>"},{"location":"helper-scripts/#oai-file-locator-pc-locate-oai","title":"OAI File Locator (pc-locate-oai)","text":"<p>Locates the OAI harvest file that contains the given FOLIO instance ID, which can be then used for importing a specific record into your stack (or re-importing it). Additionally, it has the option to extract the single record from an OAI file and put it in a temporary file. The script is available on the host machines as well as within the <code>catalog</code>, <code>cron</code> and <code>build</code> containers in the <code>catalog</code> stack.</p> <pre><code># Locate the file that contains data for in01234 in catalog-prod's OAI files\n# that have previously been imported to that environment\npc-locate-oai in01234\n# or\npc-locate-oai in01234\n\n# Give verbose output to show you the grep command being run\npc-locate-oai in01234 --debug\n\n# Locate the file that contains data for in01234 in catalog-beta's OAI files\npc-locate-oai in01234 catalog-beta\n\n# Locate the files that contain data for in00005342798 and in00001442723\n# then extract the data for those specific records into a temp file\npc-locate-oai in00005342798,in00001442723 --extract\n</code></pre>"},{"location":"helper-scripts/#hlm-file-locator-pc-locate-hlm","title":"HLM File Locator (pc-locate-hlm)","text":"<p>Locates the HLM harvest file that contains the given pattern, and display the files that the match is found in. The script is available on the host machines.</p> <pre><code># Locate the file that contains '123456789' in catalog-prod's HLM files\npc-locate-hlm catalog-prod -p 123456789\n\n# Give verbose output to show you the grep command being run\npc-locate-hlm catalog-prod -p 123456789 --debug\n</code></pre>"},{"location":"helper-scripts/#record-manipulation-pc-record","title":"Record manipulation (pc-record)","text":"<p>Helper to manipulate records.</p> <p>Currently available:</p> <ul> <li>delete: delete records from provided files and or inline ids. The script   is available on the host machines as well as within the <code>catalog</code>, <code>cron</code> and   <code>build</code> containers in the <code>catalog</code> stack.</li> </ul> <pre><code># delete the record with id hlm.in01234 on catalog-beta\npc-record delete catalog-beta hlm.in01234\n\n# delete records with ids in input file on devel-robby being verbose\npc-record delete devel-robby --input file_containing_ids.txt --debug\n\n# show the command to delete the record with id in01234\n# (folio.in01234 with prefix) on catalog-beta being verbose \npc-record delete catalog-beta in01234 --dry-run --vvv --prefix folio\n</code></pre>"},{"location":"helper-scripts/#connect-to-container-pc-connect","title":"Connect to container (pc-connect)","text":"<p>Helper to connect to a container for a given service (and optionally, on a particular node). Also has the option to override the <code>bash</code> command with anything else, or with helpers to run the <code>mysql</code> or <code>zh-shell</code> commands.</p> <pre><code># Connect to the catalog instance with verbose logging\npc-connect catalog-prod-catalog_cron -v\n\n# Connect to the database on node 3\npc-connect catalog-prod-mariadb_galera 3\n\n# Connect to zk-shell\npc-connect catalog-prod-solr_solr --zk\n\n# Dry-run to locate an instance\npc-connect devel-test-catalog_catalog -n\n</code></pre>"},{"location":"helper-scripts/#run-full-import-pc-full-import","title":"Run full import (pc-full-import)","text":"<p>Helper to run a full import of data into an environment using the folio and hlm data in their <code>processed</code> or <code>current</code> directories. This should be run in a <code>screen</code> since it will likely run for a day and needs to be run with <code>sudo</code> on the host.</p> <pre><code>screen # run in a screen, this isn't required, but highly recommended\n\n# List all of the steps the script will run\nsudo pc-full-import catalog-prod --list\n\n# Run a full import with debug output saving to a file\nsudo pc-full-import catalog-prod --debug 2&gt;&amp;1 | tee catalog-prod-import_$(date -I).log\n\n# Run only a few steps from script bypassing user confirmation\n# (if that step asks for it)\nsudo pc-full-import catalog-prod --first-step 3 --last-step 5 --debug --yes\n\n# Do a dry run of the full import to show what steps it would perform\nsudo pc-full-import catalog-prod --dry-run\n</code></pre>"},{"location":"job-schedules/","title":"Job Schedules","text":"<p>This page lists the current schedule for all the jobs that run on a regular basis.</p>"},{"location":"job-schedules/#all-cron-jobs","title":"All Cron Jobs","text":"Environment Container Job Time prod Solr Cron Alphabrowse Rebuild 1:15AM every day (build on node <code>1</code>) beta Solr Cron Alphabrowse Rebuild 2:15AM every day (build on node <code>2</code>) preview Solr Cron Alphabrowse Rebuild 3:15AM every day (build on node <code>3</code>) prod VuFind Cron FOLIO harvest &amp; Import 3AM - 11PM @ :00, :15, :30, and :45 beta VuFind Cron FOLIO harvest &amp; Import 3AM - 11PM @ :15 preview VuFind Cron FOLIO harvest &amp; Import 3AM - 11PM @ :45 prod VuFind Cron HLM harvest &amp; Import 2:30AM every day beta VuFind Cron HLM harvest &amp; Import 2:15AM every day preview VuFind Cron HLM harvest &amp; Import 2:45AM every day prod VuFind Cron Authority harvest &amp; Import 4:30AM every day beta VuFind Cron Authority harvest &amp; Import 4:15AM every day preview VuFind Cron Authority harvest &amp; Import 4:45AM every day prod VuFind Cron Course Reserves Import Every hour @ :10 beta VuFind Cron Course Reserves Import Every hour @ :20 preview VuFind Cron Course Reserves Import Every hour @ :50 all VuFind Cron Clear old VuFind searches 12:00AM every day all VuFind Cron Clear old VuFind sessions 12:15AM, 6:15AM, 6:15PM every day prod, beta, preview VuFind Cron Clear VuFind cache At container start devel-*, review-* VuFind CacheCron Clear VuFind cache At container start and every 5 minutes"},{"location":"job-schedules/#which-node-the-vufind-cron-container-lives-on-for-each-environment","title":"Which node the VuFind Cron container lives on for each environment","text":"Environment VuFind Cron Node prod <code>1</code> beta <code>2</code> preview <code>3</code> devel-*, review* <code>-</code>"},{"location":"job-schedules/#which-node-the-solr-cron-container-lives-on-for-each-environment","title":"Which node the Solr Cron container lives on for each environment","text":"Environment Solr Cron Node prod <code>1</code>, <code>2</code>, <code>3</code> beta <code>1</code>, <code>2</code>, <code>3</code> preview <code>1</code>, <code>2</code>, <code>3</code> devel-*, review* <code>1</code>, <code>2</code>, <code>3</code>"},{"location":"load-testing/","title":"Load Testing","text":"<p>Load testing can be done against your instance at any time by running the included load-test.js script, which uses k6 to submit requests with the given parameters and provide you a report with the results at the end.</p> <p>Ideally, you will want to also want to have a connection to your server while you are running these tests so that you can monitor them for CPU load (using a tool like <code>htop</code>) to see what combination of parameters is the limit your instance can handle. For example, your server might start to consistently see 100% CPU usage and return failed request responses with slow response times when you get past 200 users for the search page URL, but the same 200 users might be fine for the home page URL. This can help you focus your efforts on what pages or areas of your infrastructure to optimize and know what limits you can expect of your servers.</p> <p>The script itself has instructions for using it, but as a quick example, you could quickly run these tests locally within a Docker container:</p> <pre><code># Queries the catalog home page with 100 users over a 1 minute duration\ndocker run --rm \\\n  -v /path/to/load_test.js:/load_test.js \\\n  grafana/k6 run \\\n  -u 100 \\\n  -d 1m \\\n  /load_test.js \\\n  --env URL=\"https://catalog.lib.msu.edu\"\n</code></pre>"},{"location":"load-testing/#benchmarking","title":"Benchmarking","text":"<p>Another tool you can use if you don't want to test your site over a period of time is the Apache benchmarking tool which is built into this VuFind image and is part of most Ubuntu installations.</p> <pre><code># Request the URL 100 times, doing 10 concurrent requests\nab -l -n 100 -c 10 \"https://catalog.lib.msu.edu/\"\n</code></pre> <p>The advantage with this output is it is much simpler, so it can be quicker to get a high level picture of performance gains when comparing the before-and-after of a code change.</p>"},{"location":"mariadb/","title":"MariaDB","text":""},{"location":"mariadb/#helpers","title":"Helpers","text":"<p>To quick-connect to the database within the container (without having to look up the password from the Docker secret or CI variable), simply use the <code>connect</code> command.</p> <pre><code>docker exec -it $(docker ps -q -f name=catalog-prod-mariadb_galera) connect\n</code></pre>"},{"location":"mariadb/#re-deploying","title":"Re-deploying","text":"<p>If you ever need to re-deploy the stack, you can use the pc-deploy script.</p> <p>Make sure you run it as the deploy user so that the proper Docker container registry credentials are passed.</p> <pre><code>sudo -Hu deploy pc-deploy catalog-prod mariadb\n</code></pre>"},{"location":"mariadb/#restarting","title":"Restarting","text":"<p>If you need to restart the Galera cluster, the easiest method is to re-run the CI job that deploys the DB updates. But alternatively, you can restart the containers one-by-one. Just wait for the restarted container to be \"healthy\" before restarting the next.</p> <pre><code>docker stop $(docker ps -q -f name=catalog-prod-mariadb_galera)\n# Wait for the new container to report healthy\nwatch 'docker ps | grep catalog-prod-mariadb_galera'\n# Now repeat those two steps on the remaining nodes in the cluster\n# Once complete, run a final check on the cluster\nsudo /usr/local/ncpa/plugins/check_galera.sh catalog-prod\n</code></pre>"},{"location":"mariadb/#troubleshooting","title":"Troubleshooting","text":"<p>In the event of the database getting de-clustered, where the nodes are unable to bootstrap themselves, you will need to manually determine which node should be started up first and be the one with the most up-to-date source of data.</p> <p>To do this, first remove the database stack so that all the containers can be stopped as gracefully as possible.</p> <pre><code>docker stack rm [STACK_NAME]-mariadb\n</code></pre> <p>Then look at the MariaDb volume data on each of the nodes to determine which had the latest timestamps on its files across all the nodes. This can vary on a file-by-file basis, but generally there should be a node that is more ahead than others or a table that is more important than others and is more up-to-date (i.e. user data vs session data).</p> <pre><code># Run this on all of your nodes and compare timestamps\n\n# Particular file to look at would be: gvwstate.dat, grastate.dat, mysql-bin*\nls -ltr /var/lib/docker/volumes/[STACK_NAME]-mariadb_db-bitnami/_data/mariadb/data\n\n# Key files (tables) here are: change_tracker.ibd, user.ibd, user_list.ibd\nls -ltr /var/lib/docker/volumes/[STACK_NAME]-mariadb_db-bitnami/_data/mariadb/data/vufind\n\n# If the timestamps are too similar, try using `stat` to get a more accurate time!\nstat grastate.dat\n</code></pre> <p>This step can be tricky since some of the files may have more current timestamps on one node, but then one other node may have the most current timestamp for another particular file. Use your best judgement here. Generally the top level files are more important (the galera state files and binary logs where it tracks changes), but you also don't want to lose data from the <code>vufind</code> database. Making sure you have your backup located before attempting this would be a good idea if you are not confident in which node to pick.</p> <p>Once you have the node number you want to bring up as your source of truth, update the <code>docker-compose.mariadb-cloud-force.yml</code> file and update the <code>\"node.labels.nodeid==N\"</code> to change the <code>N</code> to you your node number, i.e. a value 1-3. Then also update the <code>max_replicas_per_node</code> to <code>1</code> to indicate that you're ready to deploy.</p> <p>Now we're ready to bring back up the stack with just the single node in bootstrap mode.</p> <pre><code>sudo -Hu deploy docker stack deploy --with-registry-auth -c &lt;(source .env; envsubst &lt;docker-compose.mariadb-cloud-force.yml) [STACK_NAME]-mariadb\ndocker service logs -f\n</code></pre> <p>Watch the logs until the state is happy and ready for connections (meaning that it will say \"ready for connections\" in the logs towards the end and stop printing messages). Then bring the stack down again, so it can be re-deployed with the regular cloud compose file. It is important to bring the stack down first so that it can cleanly stop first and disable its bootstrap state before the other nodes come online.</p> <pre><code>docker stack rm [STACK_NAME]-mariadb\n# wait for the container to stop\nsudo -Hu deploy docker stack deploy --with-registry-auth -c &lt;(source .env; envsubst &lt;docker-compose.mariadb-cloud.yml) [STACK_NAME]-mariadb\n</code></pre> <p>The stack should now come back up with all the nodes being healthy and joined to the cluster.</p> <p>Note</p> <p>Remember to restore the <code>docker-compose.mariadb-cloud-force.yml</code> file !</p>"},{"location":"mariadb/#verifying-cluster-health","title":"Verifying cluster health","text":"<p>You can verify that all nodes are joined to the cluster via the Docker service logs and scrolling up to look for the members list. There should be a list similar to:</p> <pre><code>members(3):\n    0: 1557dc68-6d5b-11ed-811e-5f59f6f49aa8, galera3\n    1: 15663de2-6d5b-11ed-b380-2abbd5ec6e2b, galera2\n    2: f48d4f88-6d5a-11ed-978c-1b318d6b5649, galera1\n</code></pre> <p>You can also verify the cluster status by connecting to the database from one of the nodes and querying for the WSREP status:</p> <pre><code>SHOW GLOBAL STATUS LIKE '%wsrep%'\\G\n</code></pre> <p>some key values are the <code>wsrep_cluster_size</code> (which should match the number of nodes you have) and the <code>wsrep_cluster_state_uuid</code> (which should be the same on all the nodes).</p> <p>Using the NCPA checks deployed via the catalog-infrastructure repository will also run many of these health checks.</p> <pre><code>sudo /usr/local/ncpa/plugins/check_galera.sh catalog-prod\n</code></pre>"},{"location":"monitoring/","title":"Monitoring Application","text":""},{"location":"monitoring/#introduction","title":"Introduction","text":"<p>The Monitoring app helps to monitor the Public Catalog system as a whole. It is a web application displaying a dashboard of the status for the different services and cron processes, doing checks across all the nodes. It also records available memory, disk space, Apache requests and response times to display graphs of these variables over time. To make troubleshooting easy, it also provides easy access to the numerous logs for the different services across all the nodes. Finally, it provides links to other admin panels. The home page is refreshed regularly (with an html <code>meta</code>).</p>"},{"location":"monitoring/#access","title":"Access","text":"<p>The Monitoring app is available with the path <code>/monitoring</code>, using the same credentials as the Traefik and Solr admin panels. Access is controlled with traefik.</p>"},{"location":"monitoring/#re-deploying","title":"Re-deploying","text":"<p>If you ever need to re-deploy the stack, you can use the pc-deploy script.</p> <p>Make sure you run it as the deploy user so that the proper Docker container registry credentials are passed.</p> <pre><code>sudo -Hu deploy pc-deploy catalog-prod monitoring\n</code></pre>"},{"location":"monitoring/#ui-sections","title":"UI Sections","text":""},{"location":"monitoring/#status","title":"Status","text":"<p>This section displays quick-glance status information for key services, jobs, and node data such as (not a complete list):</p> <ul> <li>Disk space</li> <li>VuFind</li> <li>Solr</li> <li>MariaDB</li> <li>FOLIO harvest</li> <li>Alphabetical browse update</li> <li>Backup jobs</li> </ul>"},{"location":"monitoring/#graphs","title":"Graphs","text":"<p>Links to various charts with data over time, such as memory, disk usage and response time.</p>"},{"location":"monitoring/#logs","title":"Logs","text":"<p>Links to the logs for each service and job, and each log page shows the logs for each of the nodes in the cluster. For example you can see the Solr logs for each node.</p>"},{"location":"monitoring/#other-admin-apps","title":"Other admin apps","text":"<p>This section contains links to other outside services, like the Traefik dashboard and Solr's administrative interface.</p>"},{"location":"monitoring/#json-status-for-nodes","title":"JSON status for nodes","text":"<p>The monitoring app is running on each node. The status specific to each node can be obtained with the path <code>/monitoring/node/status</code>. So for instance within a container using the docker network, one can get node 2's status with <code>http://monitoring2/monitoring/node/status</code>.</p>"},{"location":"monitoring/#implementation","title":"Implementation","text":"<p>Implementation is in Python with Flask, in Docker. The starting point is simply <code>python app/app.py</code>. It is using a mariadb database called <code>monitoring</code> (using galera like the other services).</p> <p>Here is a summary of what top-level files/directories are for:</p> <ul> <li><code>static</code>: CSS and Javascript files</li> <li><code>templates</code>: Jinja templates</li> <li><code>app.py</code>: main file, including all the routes, and starting the scheduler</li> <li><code>collector.py</code>: regular task saving the variables in a database; also   collects Apache requests and response times by looking at the access log.</li> <li><code>graphs.py</code>: functions to create graphs</li> <li><code>home.py</code>: prepares the home page template using functions in <code>status.py</code>.</li> <li><code>logs.py</code>: gathers and displays the logs; log files are read from the   <code>${STACK_NAME}_logs</code> docker volume.</li> <li><code>status.py</code>: gathers all status information</li> <li><code>util.py</code>: utilities (mainly to do async http requests in parallel with   <code>asyncio</code> and <code>aiohttp</code>)</li> </ul>"},{"location":"monitoring/#pylint-arguments","title":"Pylint arguments","text":"<ul> <li><code>--disable=missing-module-docstring</code></li> <li><code>--disable=missing-class-docstring</code></li> <li><code>--disable=missing-function-docstring</code></li> <li><code>--max-line-length=120</code></li> <li><code>--good-names=i,j,k,x,y,ex</code></li> </ul>"},{"location":"solr/","title":"Solr","text":""},{"location":"solr/#helpers","title":"Helpers","text":"<p>To quick-connect to the zk-shell within the container (without having to pass the ZK hosts) simply use the <code>solr-zk-shell</code> command.</p> <pre><code>docker exec -it $(docker ps -q -f name=catalog-prod-solr_solr) solr-zk-shell\n</code></pre>"},{"location":"solr/#re-deploying","title":"Re-deploying","text":"<p>If you ever need to re-deploy the stack, you can use the pc-deploy script.</p> <p>Make sure you run it as the deploy user so that the proper Docker container registry credentials are passed.</p> <pre><code>sudo -Hu deploy pc-deploy catalog-prod solr\n</code></pre>"},{"location":"solr/#collection-structure","title":"Collection Structure","text":"<p>Most of the collections do not have an alias associated with them. But, <code>biblio</code> is special! For that collection we are using Solr aliases because we want to be able to clear out the index completely and rebuild it without causing downtime for search on our site. The <code>biblio</code> alias will point to the \"live\" biblio collection and the <code>biblio-build</code> alias will be used for rebuilding the index on.</p> <pre><code>graph LR\n    biblio --&gt; biblio1;\n    biblio-build --&gt; biblio2;\n    subgraph Aliases;\n    biblio;\n    biblio-build;\n    end;\n    subgraph Collections;\n    biblio1;\n    biblio2;\n    end;</code></pre> <p>Can be swapped to be:</p> <pre><code>graph LR\n    biblio --&gt; biblio2;\n    biblio-build --&gt; biblio1;\n    subgraph Aliases;\n    biblio;\n    biblio-build;\n    end;\n    subgraph Collections;\n    biblio1;\n    biblio2;\n    end;</code></pre> <p>Ideally, when you complete the swap over to the <code>biblio</code> alias, you would clear out the contents of the collection that is now pointing to the <code>biblio-build</code> alias to avoid extra disk usage.</p>"},{"location":"solr/#updating-the-solr-configuration-files","title":"Updating the Solr Configuration Files","text":"<p>You should not need to do this in a new environment (schema changes should be built into the Docker image) but for development, sometimes you may want to test changes to the Solr schema. Also, during upgrades you may need to apply updates to config files to existing environments (as you likely won't want to just delete your entire volume and start over). For simplicity, we've documented how to make those updates to them here since it has to be done via one of the Solr containers and running the <code>zk</code> commands.</p> <pre><code># Copy the file(s) you need from Zookeeper\nsolr zk cp zk:/solr/configs/biblio/solrconfig.xml /tmp/solrconfig.xml -z zk1:2181\n\n# Now make your updates at the location you updated them\n\n# Finally, copy those updated files back onto Zookeeper\nsolr zk cp /tmp/solrconfig.xml zk:/solr/configs/biblio/solrconfig.xml -z zk1:2181\n</code></pre>"},{"location":"solr/#deleting-documents-from-the-solr-index","title":"Deleting documents from the Solr index","text":"<p>In the event that you need to manually remove items from one of the Solr collections, you can connect to one of the containers in your stack that have <code>curl</code> installed and are within the <code>internal</code> network (such as one of the VuFind containers) and run the following command.</p> <pre><code># Deletes the document with the id of folio.in00006795294 from the biblio collection\ncurl 'http://solr1:8983/solr/biblio/update' --data '&lt;delete&gt;&lt;query&gt;id:folio.in00006795294&lt;/query&gt;&lt;/delete&gt;' -H 'Content-type:text/xml; charset=utf-8'\ncurl 'http://solr1:8983/solr/biblio/update' --data '&lt;commit/&gt;' -H 'Content-type:text/xml; charset=utf-8'\n</code></pre>"},{"location":"solr/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Solr can be accessed via https://your-site/solr   which is helpful for verifying the state that the cloud is in   (Cloud -&gt; Nodes) as well as the ZooKeeper containers (Cloud -&gt; ZK Status).   Additionally, you can check the status of the collections (Cloud -&gt; Graph)   to make sure they are marked as Active. It may also be helpful to use the   web interface for testing queries.</p> </li> <li> <p>A helper script is included with all the nodes, called <code>clusterhealth.sh</code>,   that can be run to check all the replicas and shards across all nodes   and report and issues identified. It can be run by:</p> </li> </ul> <pre><code>docker exec $(docker ps -q -f name=${STACK_NAME}-solr_solr) /clusterhealth.sh\n</code></pre> <ul> <li>Using the NCPA checks deployed via the catalog-infrastructure repository   will also run many of these health checks.</li> </ul> <pre><code>sudo /usr/local/ncpa/plugins/check_galera.sh catalog-prod\n</code></pre> <ul> <li>To view the Docker healthcheck logs from a particular container you can:</li> </ul> <pre><code># View from running containers\ndocker inspect --format '{{ .State.Health.Status }}' $(docker ps -q -f name=${STACK_NAME}-solr_solr)\ndocker inspect --format '{{ (index .State.Health.Log 0).Output }}' $(docker ps -q -f name=${STACK_NAME}-solr_solr)\n\n# View from stopped containers\ndocker inspect --format '{{ .State.Health.Status }}' [CONTAINER_ID]\ndocker inspect --format '{{ (index .State.Health.Log 0).Output }}' [CONTAINER_ID]\n\n# Run healthcheck script manually\ndocker exec $(docker ps -q -f name=${STACK_NAME}-solr_solr) /healthcheck.sh\n</code></pre>"},{"location":"solr/#fixing-down-solr-replicas","title":"Fixing Down Solr Replicas","text":"<p>In the event that you find a Solr replica that appears to be stuck in a \"down\" state despite all efforts to bring it back online, it may be easiest to just discard that replica and recreate it.</p> <p>This can be accomplished via the <code>DELETEREPLICA</code> and <code>ADDREPLICA</code> Solr API calls. See https://solr.apache.org/guide/8_10/replica-management.html.</p> <p>For example, if one replica in a shard is stuck down, you can simply remove the downed replicas and then add a new replica to replace it.</p> <pre><code># Identified one down replicas for `biblio` on solr3 to be removed.\n# We don't have to specify solr3 here, as we're setting `onlyIfDown`.\ncurl 'http://solr:8983/solr/admin/collections?action=DELETEREPLICA&amp;collection=biblio&amp;count=1&amp;onlyIfDown=true&amp;shard=shard1'\n\n# Create a new replica for `biblio` on solr3 to replace the one we removed.\ncurl 'http://solr:8983/solr/admin/collections?action=ADDREPLICA&amp;collection=biblio&amp;shard=shard1&amp;node=solr3:8983_solr'\n</code></pre> <p>Note, the new replica may take a few minutes to \"recover\" while it comes up. This is the process where it gets current collection data from the other replicas.</p>"},{"location":"solr/#fixing-count-differences","title":"Fixing Count Differences","text":"<p>Occasionally the replicas can get out of sync and have slightly different counts between the replicas. If the leader is the one with the off-number and the other replicas are in-sync, then the solution is just to stop the container on the leader node to force the leadership to change to one of the other nodes. This will trigger the out-of-sync replica to re-sync with the new leader.</p> <p>You can verify current leadership status via the Solr Admin interface in Cloud -&gt; Graph.</p> <p>You can verify counts on each replica by doing a curl call (or viewing in your browser, replacing with your site's URL):</p> <pre><code>curl 'http://solr:8983/solr/admin/metrics?nodes=solr1:8983_solr,solr2:8983_solr,solr3:8983_solr&amp;prefix=SEARCHER.searcher.numDocs,SEARCHER.searcher.deletedDocs&amp;wt=json'\n</code></pre> <p>If every node is out of sync, then you will want to look at the volume file timestamps to determine the most recently modified as well as determining which has the highest index count. Then, to force that node to become leader, you will need to pause the other docker nodes (the ones you do NOT want to be the new leader) from accepting containers, stop the ones you don't want to be leader one at a time, so the container remaining becomes leader. Once complete, un-pause the docker nodes, so they can accept new containers again. You can watch the service logs to ensure they are recovering from the leader node and the counts sync back up over a period of time.</p> <pre><code>docker node ls\n# Do this for both nodes you need to pause\ndocker node update [ID or hostname] --availability pause\n# Verify their state\ndocker node ls\n# Re-enable them after you bring your containers back up\ndocker node update [ID or hostname] --availability active\n</code></pre>"},{"location":"solr/#running-the-alphabetical-indexing-manually","title":"Running the alphabetical indexing manually","text":"<p>Each node in <code>solr_cron</code> runs a script to build the alpha browse databases. These are offset to not run at the same time on each node (starting with node 3 for beta, node 1 for prod). The first node to run will generate the new database files and put them into the shared storage space, the other nodes will detect the recently built files and just copy them.</p> <pre><code>/alpha-browse.sh -v -p /mnt/shared/alpha-browse/$STACK_NAME\n</code></pre>"},{"location":"solr/#directory-structure-and-files-in-the-solr-image","title":"Directory structure and files in the Solr image","text":"<p>We use VuFind configurations for alphabetical indexing and browsing. Browsing is started by Solr in the <code>solr_solr</code> container, and configuration is done in the Solr config file for the biblio collection <code>solrconfig.xml</code>. Indexing is started with VuFind's <code>index-alphabetic-browse.sh</code>, which we start with the <code>alphabrowse.sh</code> script with cron in the <code>solr_cron</code> container. VuFind includes its own bundled version of Solr, so its configurations have paths reflecting that, and it doesn't translate well when using Solr in separate containers.</p> <p>The constraints are:</p> <ul> <li><code>index-alphabetic-browse.sh</code> assumes there is an <code>import</code> directory in   the same directory as itself.</li> <li>The <code>CLASSPATH</code> in <code>index-alphabetic-browse.sh</code> implies <code>browse-indexing.jar</code>   and <code>solrmarc_core*.jar</code> are in <code>${VUFIND_HOME}/import</code>, <code>marc4j*.jar</code> is in   <code>${VUFIND_HOME}/import/lib/</code> and other jars are in <code>${SOLR_HOME}/jars</code>.</li> <li>The <code>solrconfig.xml</code> file for the <code>biblio</code> collection includes relative   paths to jar libraries.</li> </ul> <p>The Solr <code>Dockerfile</code> puts VuFind <code>/solr/vufind</code> files into <code>/solr_confs</code> and customizes them. It also edits the lib paths in <code>solrconfig.xml</code>.</p> <p>To run <code>index-alphabetic-browse.sh</code>, <code>alphabrowse.sh</code> sets <code>SOLR_HOME=/tmp/alpha-browse/build</code> and <code>VUFIND_HOME=/solr_confs</code> and adds a symlink from <code>/tmp/alpha-browse/build/jars</code> to <code>/bitnami/solr/server/solr/jars/</code>.</p> <p>One thing to keep in mind when changing the structure is that the <code>/bitnami</code> directory is stored in the <code>solr_bitnami</code> Docker volume. So any change to the files in the image copied to <code>/bitnami</code> during initialization is not necessarily reflected in the volume after deployment, and it might require manual updates. We used to copy jar files into <code>/opt/bitnami/solr/server/solr/jars</code> and <code>/opt/bitnami/solr/server/solr-webapp/webapp/WEB-INF/lib/</code> and these files were copied on startup to <code>/bitnami/solr/...</code>, but these were not getting updated automatically after a change. Now that we keep files in <code>/solr_confs</code> and change the paths in <code>solrconfig.xml</code>, updates to the jars are applied automatically.</p>"},{"location":"solr/#restarting-solr","title":"Restarting Solr","text":"<p>With restarts on Solr, the important part is to make sure each node is back online and joined/synced before restarting the next one.</p> <p>Restarting is just <code>docker stop &lt;container&gt;</code> on each node, as Docker Swarm will quickly restart it as a new container. Then keeping an eye on the <code>/solr/</code> cloud nodes and graph. Note, this can be done programmatically, as we have done in our <code>check_solr.sh</code> NCPA plugin script. (TODO link to catalog-infrastructure script)</p> <p>In the <code>/solr/</code> web page, under the \"Cloud\" tab on the left there is a \"Nodes\" and \"Graph\" section. The nodes section shows if nodes are up and happy, the graph section reports on if the replicas are up and ready.</p> <p>If the logs mention being unable to elect a leader, scale to 3 nodes (so they see each other), then scale down to 1 node (last node sees other nodes leave) and then wait for a leader election timeout. The remaining node will eventually (logs will periodically report a timeout period remaining) become the leader once the timeout happens, and it stops waiting for its peers to return. Then scale back up to 2, and then 3. You can tell when they are ready to be scaled back up once they show as green in /solr -&gt; Cloud -&gt; Graph. The command to scale the nodes is: <code>docker service scale catalog-prod-solr_solr=3</code>, changing the stack name and replica count as appropriate.</p> <p>For example if <code>solr2</code> complains after all 3 nodes are restarted that it couldn't connect to <code>solr1</code>, restarting <code>solr1</code> again fixes it.</p> <pre><code>ERROR (recoveryExecutor-10-thread-3-processing-solr2:8983_solr biblio9_shard1_replica_n1 biblio9 shard1 core_node3) [c:biblio9 s:shard1 r:core_node3 x:biblio9_shard1_replica_n1] o.a.s.c.RecoveryStrategy Failed to connect leader http://solr1:8983/solr on recovery, try again\n</code></pre> <p>If a replica doesn't come online and there is no indication that is recovering in the logs, it could be a stuck leader. In which case changing who is leading that collection's replicas can allow the downed replica to come back online. In extreme cases where the replica just refuses to come back online, remove the replica and then re-add it (manually via Solr API calls using curl). This last case might be due to a split brain, so unless we are certain that isn't the case (i.e. we know no Solr updates have occurred), starting a reimport is probably a good idea.</p> <p>We can try to change a leader by forcing an election (via Solr API call). Or if we stop the node, it'll lose leader status.</p>"},{"location":"tech-overview/","title":"Technical Overview","text":""},{"location":"tech-overview/#high-level-overview","title":"High Level Overview","text":"<p>If you\u2019re looking for the answer for the quickest answer to \u201chow do we deploy our site\u201d, then you\u2019re reading the right section! We use two separate code repositories, one for infrastructure and one for our application code (this repository). The infrastructure repository is internal and uses GitLab CI/CD to kick off Ansible playbooks that run terraform commands to manage the infrastructure in AWS for us. This repository however is public, and contains all our customization on top of VuFind (like our own custom module and theme) but much more beyond that \u2013 it has our own Docker swarm setup that runs all the services VuFind depends on.</p> <p>Just like our infrastructure repository, our application repository uses GitLab CI/CD to run jobs that deploy the code to our AWS EC2 instances. Based on the Git branch name, it will decide if it will spin up a new development environment, update an existing one, or update our production environment.</p> <p>To get a little more into the details, the CI/CD triggers a Docker build passing <code>--build-arg</code> parameters to the build command with multiple CI/CD variables stored in GitLab. Those variables are then used in the Dockerfile throughout the build (for example: the <code>config.ini</code> file is populated with data in the CI/CD variables with the use of the <code>envsubst</code> command in the Dockerfile after the <code>ARG</code> variables have been copied into <code>ENV</code> variables).</p>"},{"location":"tech-overview/#detailed-overview","title":"Detailed Overview","text":""},{"location":"tech-overview/#infrastructure-as-code","title":"Infrastructure as Code","text":"<ul> <li>No reliance on manually provisioning hardware or waiting for humans to   accomplish any task. From nothing to a full production ready environment   can be accomplished via the CI tasks.</li> <li>Using Terraform to provision entire stack:<ul> <li>Virtual Networking &amp; Firewalling</li> <li>Shared services like mounted storage and email relay service</li> <li>Virtual Machines with Network Interfaces, IPs, &amp; Local Block Storage</li> <li>Initial user setup</li> </ul> </li> <li>Once provisioning is completed, all users and core services are ready   for use.</li> </ul>"},{"location":"tech-overview/#fully-redundant","title":"Fully Redundant","text":"<ul> <li>Current infrastructure spans 3 availability zones (i.e. different data   centers) and could expand to allow for additional nodes.</li> <li>Each service is clustered with active load balancing; this includes   MariaDB, Solr, and VuFind (along with their supporting services, like   cron jobs). The public access point (Traefik) is a lightweight single   instance which Docker Swarm can redeploy onto another node in seconds,   should its current node go down.</li> </ul>"},{"location":"tech-overview/#automated-environment-creation-and-destruction","title":"Automated Environment Creation (and Destruction)","text":"<ul> <li>With an appropriately named git branch, a fully functional environment   will be created for use or testing.<ul> <li>Creating a branch starting with <code>devel-</code> or <code>review-</code> will   automatically trigger CI stages to create a new deployment site.</li> <li>GitLab CI also has 1 click clean-up and removal of environments   (except production, to prevent mistakes); running the clean-up   will free the resources on the server, and will not remove the   source code files, nor delete the git branch.</li> </ul> </li> <li>Environments are completely disposable. We typically make one for a   ticket, then destroy it once we close it. The only ones we leave up   are the production deployments (currently, our \"beta\", \"prod\" and   \"preview\" sites).</li> </ul>"},{"location":"tech-overview/#equality-between-development-and-production-environments","title":"Equality Between Development and Production Environments","text":"<p>Anything the production environment has, the development environments have as well. Full redundant services, TLS Certificates from Let's Encrypt, email support, and so on. The only thing we decided to limit was the automated data import. That was restricted to 10,000 records just because importing the full catalog would slow things down too much (approx 9 million bib records + 3 million authority records).</p>"},{"location":"tech-overview/#idempotency-for-both-infrastructure-and-environment-deployment","title":"Idempotency for both Infrastructure and Environment Deployment","text":"<p>We run the same CI pipelines for both the creation &amp; modification of our infrastructure. Likewise for our deployment environments, one unified pipeline that can run to create or update things.</p>"},{"location":"tech-overview/#developer-friendly-interfaces-logs","title":"Developer Friendly Interfaces &amp; Logs","text":"<ul> <li>Services with management or monitoring interfaces are automatically   enabled with permissions granted so developers can access them.<ul> <li>Solr Admin Web Interface</li> <li>Traefik Dashboard</li> <li>Locally developed service monitoring dashboard</li> </ul> </li> <li>Logs for services are set up to be output to Docker for ease of accessing.</li> </ul>"},{"location":"tech-overview/#integrations-diagram","title":"Integrations Diagram","text":"<p>The below diagram describes the external systems the VuFind integrates with. This is not an exhaustive list of all the technologies used within this stack (see the section later in this page for that), but should represent the systems that VuFind gets and sends data to on a regular basis.</p> <pre><code>graph LR\n\nPatron((Patron))\nFOLIO[FOLIO]\nBackStage[\"`BackStage FTP Server\n(authority records)`\"]\nMonitoring[Monitoring App]\nLogs@{ shape: docs, label: \"Logs\"}\nMariaDB[(MariaDB)]\nSolr[Solr]\nIlliad[\"Illiad\"]\nMatomo[Matomo]\n\nFOLIO --&gt; VuFindCron\nHLM --&gt; VuFindCron\nBackStage --&gt; VuFindCron\n\nPatron &lt;--&gt; VuFindWeb\n\nVuFindWeb &lt;--&gt; FOLIO\nVuFindWeb &lt;--&gt; EDSAPI\nVuFindWeb &lt;--&gt; CoverImages\nVuFindWeb --&gt; Illiad\nVuFindWeb --&gt; Matomo\n\nVuFind &lt;--&gt; MariaDB\nVuFind &lt;--&gt; Solr\n\n\nVuFind --&gt; Logs\nMariaDB --&gt; Logs\nSolr --&gt; Logs\nLogs --&gt; Monitoring\n\nsubgraph VuFind[\"**VuFind**\"]\nVuFindWeb[Web Application]\nVuFindCron[Cron]\nend\n\nsubgraph EDS[\"**EDS**\"]\nHLM[\"`FTP Server\n(holding link management records)`\"]\nEDSAPI[API]\nend\n\nsubgraph CoverImages[\"**Cover Images**\"]\nSyndetics[Syndetics]\nBrowZine[BrowZine]\nGoogle[Google]\nend</code></pre>"},{"location":"tech-overview/#technologies-used","title":"Technologies Used","text":"<ul> <li>Ansible Playbooks: Automates the provisioning and configuration of the   setup of the EC2 nodes and the Docker Swarm stacks and provision DNS names   as needed for development and review environments (some of which are stored   in an internal repository)</li> <li>AWS Cloud Services: AWS self-service resources are used to allow for full   infrastructure-as-code and automation of the provisioning</li> <li>Docker: Used to create images for the various services (VuFind, Solr,   etc.) that containers are created from</li> <li>Docker Swarm: Manages the server \"nodes\" and creates containers based   on our server definitions</li> <li>GitHub Pages: The hosting service used for this documentation</li> <li>GitLab CI/CD:  The key tool our tool belt that allows us to define highly   customized pipelines to provision and configure our application stack</li> <li>GitLab Container Registry: Stores the Docker images built, which our EC2   instances have a read-only access key to the registry to pull from</li> <li>LetsEncrypt: Provides automatically provisioned SSL certificates based on   settings in our Docker swarm configuration file</li> <li>marc-utils: For MARC   file manipulation</li> <li>MariaDB Galera:  A synchronous database cluster providing higher   availability and more fault tolerance</li> <li>MkDocs: The static site generator used to generate this documentation</li> <li>Nginx: Handles proxying requests to <code>/solr</code> to the Solr container.   Allowing us to keep the Solr containers only on the internal network but   still being able to access the Solr interface via the web</li> <li>SolrCloud: A more fault-tolerant and highly available search and indexing   service than just traditional Solr, distributing index data across multiple   nodes</li> <li>Terraform: Handles the provisioning and state management of the AWS   cloud services used, such as EC2, IAM, Route53, and EFS (some of which   are stored in an internal repository)</li> <li>Traefik: Use to route traffic externally to the appropriate VuFind   container; and also used for an internal network of the MariaDB service</li> <li>VuFind: What we're all here for! VuFind is the core application all   of this infrastructure is all built trying to serve</li> <li>ZooKeeper: Used by SolrCloud to manage configuration files for   the collections</li> </ul>"},{"location":"tech-overview/#docker-swarm-stacks-and-services","title":"Docker Swarm Stacks and Services","text":"<ul> <li>catalog:<ul> <li>catalog: Runs VuFind with Apache in the foreground</li> <li>cron: A single replica service using the <code>catalog</code> service's   image to run automated jobs   such as the periodic harvest and import of data from FOLIO</li> <li>legacylinks: Redirects legacy Sierra formatted URLS to VuFind   record pages</li> <li>croncache: Only on development environments, clearing local cache   files that are created</li> </ul> </li> <li>solr:<ul> <li>solr: Runs SolrCloud</li> <li>zk: ZooKeeper in the foreground</li> <li>cron: Runs automated jobs using the SolrCloud image (updating the   alphabetical browse databases)</li> <li>proxysolr:  Runs Nginx to proxy requests from the public network to   the Solr container</li> </ul> </li> <li>mariadb:<ul> <li>galera: Runs MariaDB Galera</li> </ul> </li> <li>internal:<ul> <li>health: Creates only the internal network used by the <code>galera</code>   service</li> </ul> </li> <li>traefik:<ul> <li>traefik:  Runs Traefik and handles external traffic and routes it   to the appropriate <code>catalog</code> service depending on the host name of the   request (since multiple environments run in separate stacks on the same   Docker swarm)</li> </ul> </li> <li>public:<ul> <li>health: Creates only the public network used by the <code>catalog</code> service   and the <code>proxy</code>* services</li> </ul> </li> <li>swarm-cleanup:<ul> <li>prune-nodes: Runs a <code>docker system prune</code> with flags on each of the   nodes in the swarm</li> </ul> </li> <li>monitoring:<ul> <li>monitoring: Runs the locally developed Flask application in the   foreground to monitor the other stacks</li> <li>proxymon: Exposes the <code>monitoring</code> service publicly since that service   is only on the internal network</li> </ul> </li> </ul>"},{"location":"testing/","title":"Writing &amp; Running Tests","text":""},{"location":"testing/#writing-tests","title":"Writing Tests","text":"<p>VuFind has extensive documentation on writing unit tests for your custom code. Particularly note the <code>Related Video</code> links at the bottom, which are very helpful in getting started.</p>"},{"location":"testing/#running-tests","title":"Running Tests","text":"<p>We have included in this repository a script to run the commands for both unit tests and code quality tests.</p> <pre><code># From within a running VuFind container\nrun-tests\n</code></pre>"},{"location":"testing/#coverage-tests","title":"Coverage Tests","text":"<p>To get the current coverage status:</p> <pre><code># From within a running VuFind container\nget-coverage-summary\n</code></pre> <p>You can also locally view coverage progress in an html page by running an image on your computer.</p> <pre><code># Build the image locally\nDOCKER_BUILDKIT=1 docker build --build-arg BUILDKIT_INLINE_CACHE=1 --build-arg VUFIND_VERSION=\"9.1.2\" --build-arg SIMPLESAMLPHP_VERSION=\"2.1.1\" --tag validate vufind/\n# Run the image locally\ndocker run --rm -it -v $(pwd)/vufind/module/Catalog:/usr/local/vufind/module/Catalog -v /tmp/coverage:/usr/local/vufind/coverage validate bash\n# Update phpunit settings\nmv module/Catalog/tests/vufind_phpunit.xml module/VuFind/tests/phpunit.xml\n# Generate the coverage report\nXDEBUG_MODE=coverage vendor/bin/phing phpunitfaster -D \"phpunit_extra_params=/usr/local/vufind/module/Catalog/tests/unit-tests/ --coverage-html coverage\"\n</code></pre> <p>Now go to your browser at file:///tmp/coverage/index.html to view the interactive report to easily identify gaps.</p> <p>That same locally built docker image can be used to run the code quality tests as well as the unit tests.</p> <pre><code>docker run --rm -it -v $(pwd)/vufind/module/Catalog:/usr/local/vufind/module/Catalog -v /tmp/coverage:/usr/local/vufind/coverage validate bash\nrun-tests\n</code></pre>"},{"location":"traefik/","title":"Traefik","text":""},{"location":"traefik/#re-deploying","title":"Re-deploying","text":"<p>If you ever need to re-deploy the stack, you can use the pc-deploy script.</p> <p>Make sure you run it as the deploy user so that the proper Docker container registry credentials are passed.</p> <pre><code>sudo -Hu deploy pc-deploy core-stacks traefik\n</code></pre>"},{"location":"traefik/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Your first line of defense when debugging issues with Traefik is navigating to the Traefik dashboard at https://your-site/dashboard/ where you can see all the routers and services that have been defined. This is helpful when the issue is a configuration issue either in the Traefik command or labels.</p> </li> <li> <p>When you have basic authentication enabled, ensure that the password hash has an appropriate cost setting; 9 or less might make brute forcing easier, while 12 or higher will add significant amounts of CPU load to Traefik, causing page loads to be extremely slow. A setting of 10 is recommended.</p> </li> </ul> <pre><code>htpasswd -n -B -C 10 mylogin\n</code></pre> <ul> <li>To debug performance issues in Traefik, you can enable debug mode by adding to the traefik service: <code>--api.debug=true</code>. This enables all the debug endpoints.</li> </ul> <pre><code>curl -u user:passwd https://your-site/debug/pprof/heap -o heap.pprof\ncurl -u user:passwd https://your-site/debug/pprof/profile -o profile.pprof\ncurl -u user:passwd https://your-site/debug/pprof/block -o block.pprof\ncurl -u user:passwd https://your-site/debug/pprof/mutex -o mutex.pprof\ncurl -u user:passwd https://your-site/debug/pprof/goroutine -o goroutine.pprof\n\n# Install Go\napt install golang\n# Install pprof\ngo install github.com/google/pprof@latest\n\ngo tool pprof -top heap.pprof\ngo tool pprof -top profile.pprof\ngo tool pprof -top block.pprof\ngo tool pprof -top mutex.pprof\ngo tool pprof -top goroutine.pprof\n</code></pre>"},{"location":"traefik/#resetting-the-lets-encrypt-config","title":"Resetting the Let's Encrypt Config","text":"<p>Periodically you may want to reset the Let's Encrypt config to clear out old certificates for sites that no longer exist. This is the process you will want to use that will ensure there is limited downtime.</p>"},{"location":"traefik/#on-the-development-nodes","title":"On the development nodes","text":"<ul> <li>Create temporary node labels to identify which node should be taken offline.   Start with taking the 3rd node offline.</li> </ul> <pre><code>docker node update --label-add deployglobal=true catalog-1-dev.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-2-dev.aws.lib.msu.edu\ndocker node update --label-add deployglobal=false catalog-3-dev.aws.lib.msu.edu\n</code></pre> <ul> <li>Create a temporary service constraint to only allow the nodes with the label   value of <code>true</code> to be online.</li> </ul> <pre><code>docker service update --constraint-add 'node.labels.deployglobal == true' traefik_traefik\n</code></pre> <ul> <li>On the 3rd node, clear out the Let's Encrypt config</li> </ul> <pre><code>mkdir -p /tmp/acme_backup/\nmv /var/lib/docker/volumes/traefik_traefik/_data/*.json /tmp/acme_backup\n</code></pre> <ul> <li>Swap the label values to take the 2nd node offline</li> </ul> <pre><code>docker node update --label-add deployglobal=true catalog-1-dev.aws.lib.msu.edu\ndocker node update --label-add deployglobal=false catalog-2-dev.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-3-dev.aws.lib.msu.edu\n</code></pre> <ul> <li>On the 2nd node, clear out the Let's Encrypt config</li> </ul> <pre><code>mkdir -p /tmp/acme_backup/\nmv /var/lib/docker/volumes/traefik_traefik/_data/*.json /tmp/acme_backup\n</code></pre> <ul> <li>Swap the label values to take the 1st node offline</li> </ul> <pre><code>docker node update --label-add deployglobal=false catalog-1-dev.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-2-dev.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-3-dev.aws.lib.msu.edu\n</code></pre> <ul> <li>On the 1st node, clear out the Let's Encrypt config</li> </ul> <pre><code>mkdir -p /tmp/acme_backup/\nmv /var/lib/docker/volumes/traefik_traefik/_data/*.json /tmp/acme_backup\n</code></pre> <ul> <li>Remove the labels and temporary service constraint</li> </ul> <pre><code>docker service update --constraint-rm 'node.labels.deployglobal == true' traefik_traefik\ndocker node update --label-rm  deployglobal catalog-1-dev.aws.lib.msu.edu\ndocker node update --label-rm  deployglobal catalog-2-dev.aws.lib.msu.edu\ndocker node update --label-rm  deployglobal catalog-3-dev.aws.lib.msu.edu\n</code></pre>"},{"location":"traefik/#on-the-production-nodes","title":"On the production nodes","text":"<ul> <li> <p>Update the local DNS entries that are pointing to the <code>.aws.lib.msu.edu</code>   sites and have them point to the 1st node and wait for DNS to update</p> </li> <li> <p>On the 1st node, we will need to prevent traefik from running temporarily   while we quickly remove the certificate files. DO THIS QUICKLY   TO LIMIT DOWNTIME.</p> </li> </ul> <pre><code>docker node update --label-add deployglobal=false catalog-1.aws.lib.msu.edu\ndocker node update --label-add deployglobal=false catalog-2.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-3.aws.lib.msu.edu\ndocker service update --constraint-add 'node.labels.deployglobal == true' traefik_traefik\n# Wait for the traefik container to stop\nwatch 'docker ps | grep traefik'\n\nmkdir -p /tmp/acme_backup/\nmv /var/lib/docker/volumes/traefik_traefik/_data/*.json /tmp/acme_backup\ndocker node update --label-add deployglobal=true catalog-1.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-2.aws.lib.msu.edu\ndocker node update --label-add deployglobal=false catalog-3.aws.lib.msu.edu\ntail -f /var/lib/docker/volumes/traefik_logs/_data/traefik.log -n10\n\n## If logs are unhealthy or unable to create new certificates, restore previous ones\ncp /tmp/acme_backup/* /var/lib/docker/volumes/traefik_traefik/_data/\n</code></pre> <ul> <li> <p>Update local DNS to point now to the 2nd node and wait for DNS to update</p> </li> <li> <p>On the 2nd node, prevent traefik from running temporarily to remove the old   certificate files.</p> </li> </ul> <pre><code>docker node update --label-add deployglobal=true catalog-1.aws.lib.msu.edu\ndocker node update --label-add deployglobal=false catalog-2.aws.lib.msu.edu\ndocker node update --label-add deployglobal=false catalog-3.aws.lib.msu.edu\n# Wait for the traefik container to stop\nwatch 'docker ps | grep traefik'\n\nmkdir -p /tmp/acme_backup/\nmv /var/lib/docker/volumes/traefik_traefik/_data/*.json /tmp/acme_backup\ndocker node update --label-add deployglobal=false catalog-1.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-2.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-3.aws.lib.msu.edu\ntail -f /var/lib/docker/volumes/traefik_logs/_data/traefik.log -n10\n\n## If logs are unhealthy or unable to create new certificates, restore previous ones\ncp /tmp/acme_backup/* /var/lib/docker/volumes/traefik_traefik/_data/\n</code></pre> <ul> <li> <p>Update local DNS to point now to the 3rd node and wait for DNS to update</p> </li> <li> <p>On the 3rd node, prevent traefik from running temporarily to remove the old   certificate files.</p> </li> </ul> <pre><code>docker node update --label-add deployglobal=true catalog-1.aws.lib.msu.edu\ndocker node update --label-add deployglobal=false catalog-2.aws.lib.msu.edu\ndocker node update --label-add deployglobal=false catalog-3.aws.lib.msu.edu\n# Wait for the traefik container to stop\nwatch 'docker ps | grep traefik'\n\nmkdir -p /tmp/acme_backup/\nmv /var/lib/docker/volumes/traefik_traefik/_data/*.json /tmp/acme_backup\ndocker node update --label-add deployglobal=true catalog-1.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-2.aws.lib.msu.edu\ndocker node update --label-add deployglobal=true catalog-3.aws.lib.msu.edu\ntail -f /var/lib/docker/volumes/traefik_logs/_data/traefik.log -n10\n\n## If logs are unhealthy or unable to create new certificates, restore previous ones\ncp /tmp/acme_backup/* /var/lib/docker/volumes/traefik_traefik/_data/\n</code></pre> <ul> <li>Remove the temporary constraint and labels</li> </ul> <pre><code>docker service update --constraint-rm 'node.labels.deployglobal == true' traefik_traefik\ndocker node update --label-rm  deployglobal catalog-1.aws.lib.msu.edu\ndocker node update --label-rm  deployglobal catalog-2.aws.lib.msu.edu\ndocker node update --label-rm  deployglobal catalog-3.aws.lib.msu.edu\n</code></pre>"},{"location":"upgrading/","title":"Upgrading","text":""},{"location":"upgrading/#vufind","title":"VuFind","text":"<p>For general documentation on how to upgrade VuFind, see the official documentation. This documentation will focus on how to upgrade specific to this environment setup.</p> <ul> <li> <p>Clone the VuFind repository   locally as well as this repository   and check out the VuFind repository to the release tag that your environment   is currently on.</p> </li> <li> <p>Run the upgrade-helper.sh   script to determine what config files in the local directory need to be   updated with changes made to core files and apply them manually to   the catalog repository   in the <code>/vufind/local</code> directory. An example of running the command might be:   <code>~/catalog/vufind/upgrade-helper.sh --target-release v10.1.1   --current-release v10.0.1 --core-vf-path ~/vufind --msul-vf-path   ~/catalog/vufind -v</code>.</p> </li> <li> <p>Update the CI/CD Config   to update the <code>VUFIND_VERSION</code> variable to be the new release you   are updating to.</p> </li> <li> <p>If updating SimpleSAMLphp, also update <code>SIMPLESAMLPHP_VERSION</code> in the   same file.</p> </li> <li> <p>Create a new branch with these changes named either <code>review-</code>* or <code>devel-</code>*   to trigger a pipeline with a new environment.</p> </li> <li> <p>Make sure the <code>--data</code> parameters contains the right field in   <code>solr_cache_warmup</code> in <code>pc-import-folio</code></p> </li> <li> <p>Once the pipeline completes successfully, verify that the site loads   and works correctly.</p> </li> <li> <p>In order to test that a database migration will work correctly, take   a fresh database dump of an environment at the older version and load   it into the new release environment. Now connect to the <code>catalog</code> container   and modify the <code>config.ini</code> file to set the <code>autoConfigure</code> value to <code>true</code>   temporarily. This will enable the [URL]/Upgrade/Home URL to be accessible   to run the database migration manually. It will likely prompt for the database   credentials, which can be found in the   docker-compose.mariadb-cloud.yml   file within the environment variables. Remember to disable the   <code>autoConfigure</code> once complete. Then ensure that everything still works   post-upgrade and that data is preserved.</p> </li> <li> <p>Once thorough testing is complete, take a backup of the database on   <code>main</code>, merge the branch into <code>main</code>, then repeat the database migration   steps once the pipeline completes.</p> </li> <li> <p>It is recommended to do a re-index of Solr to apply the latest schema   changes, if the helper script detected any. In order to do this, you will   need to run the pc-import-folio script   copying back the last full harvest and doing only an import:</p> </li> </ul> <pre><code>sudo screen\npc-full-import catalog-prod --yes --debug 2&gt;&amp;1 | tee /mnt/shared/logs/catalog-prod-import_$(date -I).log\n</code></pre>"},{"location":"upgrading/#verification","title":"Verification","text":"<p>This section includes some things that should be checked after doing a VuFind upgrade.</p> <ul> <li> <p>Inside the container, execute <code>run-tests</code> to make sure all tests and linting   checks are passing.</p> </li> <li> <p>Import course reserves (via the optional CI job) to make sure that searching   and display continues to work.</p> </li> <li> <p>Verify that the \"bound with\" relationships are still working as expected.   Here are some sample records from our environment:</p> </li> </ul> <pre><code>https://catalog.lib.msu.edu/Record/folio.in00000717323\nhttps://catalog.lib.msu.edu/Record/folio.in00000336743\nhttps://catalog.lib.msu.edu/Record/folio.in00000280877\n</code></pre> <ul> <li> <p>Verify that the status displayed on the record page and search results page matches what is in the \"Get This\" button status.</p> </li> <li> <p>Check the login</p> </li> <li> <p>Check the feedback form submits (you can check result at /mail/ on devel instances)</p> </li> <li> <p>Check holdings status (Available, Checked out, ...)</p> </li> <li> <p>Check import scripts</p> </li> <li> <p>Check placing and canceling a request. Specifically check some \"bound with\"   records that have previously had problems:</p> </li> </ul> <pre><code>https://catalog.lib.msu.edu/Record/folio.in00005032730\n</code></pre> <ul> <li>Confirm there are no errors/warnings we should address in the VuFind   or Apache logs</li> </ul>"},{"location":"upgrading/#solr","title":"Solr","text":"<p>If it is just the Solr version that is being upgraded, then updates to the Docker image will handle the update. But if the schema or solr config are being updated you'll likely need to follow these steps. Potentially all you may need is to upload the new configs , but there is the chance then when you attempt to index new items into Solr it will not want to overwrite the existing index. In that case you will need to start with an empty index. You can either clear out your current index (using the <code>--reset-solr</code> flag in the <code>pc-import-folio</code> script), but this will result in downtime for your site where there will be no results returned for a period of time, or follow the below steps to build a temporary alternate collection to index in and then swap over to once it has completed. All of these commands should be run from within the Solr containers of the stack you are wanting to upgrade the Solr schema/config on.</p> <ul> <li>Get the latest config files from GitHub for the version tag you are upgrading to and <code>wget</code> them on the solr container to a temp directory. For example:</li> </ul> <pre><code>mkdir /tmp/biblio9\ncd /tmp/biblio9\ncp -r /solr_confs/biblio/conf/* /tmp/biblio9/\nrm /tmp/biblio9/s*.xml\nwget https://raw.githubusercontent.com/vufind-org/vufind/v9.0.2/solr/vufind/biblio/conf/solrconfig.xml\nwget https://raw.githubusercontent.com/vufind-org/vufind/v9.0.2/solr/vufind/biblio/conf/schema.xml\n</code></pre> <ul> <li>Update the location the index data will save to in the <code>solrconfig.xml</code>:</li> </ul> <pre><code>&lt;dataDir&gt;${solr.solr.home:./solr}/biblio&lt;/dataDir&gt;\n</code></pre> <ul> <li>Upload the config to the Zookeepers:</li> </ul> <pre><code>solr zk upconfig -confname biblio9 -confdir /tmp/biblio9 -z $SOLR_ZK_HOSTS/solr\n</code></pre> <ul> <li>Create the new collection:</li> </ul> <pre><code>curl \"http://solr:8983/solr/admin/collections?action=CREATE&amp;name=biblio9&amp;numShards=1&amp;replicationFactor=3&amp;wt=xml&amp;collection.configName=biblio9\"\n</code></pre> <ul> <li> <p>Update the location in the local <code>import.properties</code> as well on the   <code>catalog_cron</code> container, modifying the <code>local/import/import.properties</code>   file replacing the <code>biblio</code> collection references to the new collection   (i.e. <code>biblio9</code> for example). The references should be in the   <code>solr.core.name</code> and the <code>solr.hosturl</code>.</p> </li> <li> <p>Index data into the new collection from the cron container where you   modified the <code>import.properties</code> file in the previous step. Be sure to   prepare the data you wish to import as described in the   full import documentation.</p> </li> </ul> <pre><code>/usr/local/bin/pc-import-folio --verbose --collection biblio9 --batch-import\n/usr/local/bin/pc-import-hlm -i\n</code></pre> <ul> <li> <p>You can optionally re-run the reserves cron at this time too in order to get   that up to date with the current set of instance IDs, otherwise it may   potentially be out of sync until the nightly run and some course reserve   search results may have inconsistent counts.</p> </li> <li> <p>Then verify the index in Solr and in VuFind by manually updating   the <code>config.ini</code> to point to the new collection name temporarily.   Once satisfied with the output, move on to the next step.</p> </li> <li> <p>Once you are confident in the new index, remove the original index:</p> </li> </ul> <pre><code>curl \"http://solr:8983/solr/admin/collections?action=DELETE&amp;name=biblio\"\n</code></pre> <ul> <li>Create a new alias with the name <code>biblio</code> pointing to the new collection   which will direct all queries to the new collection instead of the original   one:</li> </ul> <pre><code>curl \"http://solr:8983/solr/admin/collections?action=CREATEALIAS&amp;name=biblio&amp;collections=biblio9\"\n</code></pre>"},{"location":"vufind/","title":"VuFind","text":""},{"location":"vufind/#helpers","title":"Helpers","text":"<p>Within the VuFind container, there are a few custom helper scripts that have been written to make certain tasks easier.</p> <ul> <li><code>clear-vufind-cache</code>: Clears the VuFind cache directory; which is helpful    when applying theme or language file changes.</li> <li><code>run-tests</code>: Run the unit tests and code quality tests as they would be   run within the CI/CD.</li> </ul>"},{"location":"vufind/#re-deploying","title":"Re-deploying","text":"<p>If you ever need to re-deploy the stack, you can use the pc-deploy script.</p> <p>Make sure you run it as the deploy user so that the proper Docker container registry credentials are passed.</p> <pre><code>sudo -Hu deploy pc-deploy catalog-prod catalog\n</code></pre>"},{"location":"vufind/#troubleshooting","title":"Troubleshooting","text":"<p>VuFind's troubleshooting page is a good reference for debugging options, but we'll highlight a few specifics we've found helpful.</p> <ul> <li>The first spot to check for errors is the Docker service logs which contain both Apache error and access logs (these logs can help identify critical issues such as the inability to connect to the database) as well as VuFind's application logs. To view service logs:</li> </ul> <pre><code>docker service logs -f ${STACK_NAME}-catalog_catalog\n</code></pre> <ul> <li> <p>To enable debug messages to be printed to the service logs, update the <code>file</code> setting under the <code>[Logging]</code> section in the <code>local/config/vufind/config.ini</code> file to include <code>debug</code>. For example: <code>file = /var/log/vufind/vufind.log:alert,error,notice,debug</code></p> <p>Warning with <code>debug=true</code></p> <p>You can set <code>debug=true</code> in the main <code>config.ini</code> file to have debug messages to be printed to the page instead of the service logs, but be warned that sometime too much information is printed to the page when <code>debug</code> is set to <code>true</code>, such as passwords used for ILS calls (hopefully this can be fixed to remove that from debug messages entirely).</p> </li> <li> <p>To enable higher verbosity in the Apache logging, you can update the <code>LogLevel</code> in the <code>/etc/apache2/sites-enabled/000-default.conf</code> file and then run <code>apachectl graceful</code> to apply the changes.</p> </li> <li> <p>To enable stack traces to be shown on the page when ever any exception is thrown, edit the <code>local/httpd-vufind.conf</code> file and uncomment the line that says: <code>SetEnv VUFIND_ENV development</code>. Write the changes to the file and then run <code>apachectl graceful</code> to apply the change.</p> </li> <li> <p>To clear VuFind's caches (which can sometimes hang on to old compiled stylesheets or language files), you can run the following within any VuFind container:</p> </li> </ul> <pre><code>clear-vufind-cache\n</code></pre>"}]}