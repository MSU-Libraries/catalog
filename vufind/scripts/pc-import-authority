#!/bin/bash

set -e

SCRIPT_NAME="$(basename "$0")"

# TODO References to .MRC should be more flexible to handle .mrc, .marc, .MRC, .MARC

# Set defaults
default_args() {
    declare -g -A ARGS
    ARGS[HARVEST]=0
    ARGS[FULL]=0
    ARGS[IMPORT]=0
    ARGS[LIMIT_BY_DELETE]=
    ARGS[VUFIND_HARVEST_DIR]=/usr/local/vufind/local/harvest/authority
    ARGS[FTP_SERVER]=ftp.bslw.com
    ARGS[FTP_DIR]=out/
    ARGS[FTP_USER]=${AUTH_FTP_USER}
    ARGS[FTP_PASSWORD]=${AUTH_FTP_PASSWORD}
    ARGS[SHARED_DIR]=/mnt/authority
    ARGS[SOLR_URL]="http://solr:8983/solr"
    ARGS[RESET_SOLR]=0
    ARGS[QUICK]=0
    ARGS[VERBOSE]=0
    ARGS[BYPASS_PREPROCESS]=0
    ARGS[IGNORE_FILE]=/mnt/shared/authority/ignore_patterns.txt
    ARGS[NICENESS]=19
    declare -g -a TAGS=( 100 110 111 130 150 151 155 )
}
default_args


# Script help text
run_help() {
    echo ""
    echo "Usage: Harvest authority records from via the FTP server"
    echo "       and import that data into VuFind's authority Solr index."
    echo ""
    echo "Examples: "
    echo "   ${SCRIPT_NAME} --harvest --full --import"
    echo "     Do a full harvest from scratch and import that data"
    echo "   ${SCRIPT_NAME} --harvest --import"
    echo "     Do an update harvest with changes made since the"
    echo "     last run, and import that data"
    echo "   ${SCRIPT_NAME} --import"
    echo "     Run only a import of data that has already been"
    echo "     harvested and saved to the shared location."
    echo "   ${SCRIPT_NAME} --harvest"
    echo "     Only run the harvest, but do not proceed to import"
    echo "     the data into VuFind"
    echo ""
    echo "FLAGS:"
    echo "  -t|--harvest"
    echo "      Run an harvest into VUFIND_HARVEST_DIR. Will attempt"
    echo "      to resume from last harvest state unless -f flag given."
    echo "  -f|--full"
    echo "      Forces a reset of VUFIND_HARVEST_DIR, resulting"
    echo "      in a full harvest. Must be used with --harvest."
    echo "  -i|--import"
    echo "      Run VuFind batch import on files within VUFIND_HARVEST_DIR."
    echo "  -p|--ignore-file PATH"
    echo "      Path to a newline separated file with substrings to match which will"
    echo "      be ignored when harvesting."
    echo "      Default ${ARGS[IGNORE_FILE]}"
    echo "  -B|--bypass-preprocess"
    echo "      Bypass the preprocessing step of this script which splits"
    echo "      the files into separate tag files. NOTE: the import will"
    echo "      still rely on the .{TAG}.xml in the filename to determine"
    echo "      what record type to import the file as."
    echo "  -X|--limit-by-delete IMPORT_COUNT"
    echo "      Usable with --batch-import only. This will limit the number"
    echo "      of files imported from VUFIND_HARVEST_DIR by deleting XML"
    echo "      import files exceeding the given count prior to importing."
    echo "  -d|--vufind-harvest-dir VUFIND_HARVEST_DIR"
    echo "      Full path to the VuFind harvest directory."
    echo "      Default: ${ARGS[VUFIND_HARVEST_DIR]}"
    echo "  -s|--shared-harvest-dir SHARED_DIR"
    echo "      Full path to the shared storage location for HLM files."
    echo "      Default: ${ARGS[SHARED_DIR]}"
    echo "  -S|--solr SOLR_URL"
    echo "      Base URL for accessing Solr (only used for --reset-solr)."
    echo "      Default: ${ARGS[SOLR_URL]}"
    echo "  -F|--ftp-server FTP_SERVER"
    echo "      FTP server that contains the authority records"
    echo "      Default: ${ARGS[FTP_SERVER]}"
    echo "  -D|--ftp-dir FTP_DIR"
    echo "      Directory on the FTP server that contains the authority records"
    echo "      Default: ${ARGS[FTP_DIR]}"
    echo "  -U|--ftp-user FTP_USER"
    echo "      User for connecting to the FTP server"
    echo "      Default: Stored in the environment variable \$AUTH_FTP_USER"
    echo "  -P|--ftp-password FTP_PASSWORD"
    echo "      Password for connecting to the FTP server"
    echo "      Default: Stored in the environment variable \$AUTH_FTP_PASSWORD"
    echo "  -r|--reset-solr"
    echo "      Clear out the authority Solr collection prior to importing."
    echo "  -q|--quick"
    echo "      Skip the countdown delays before each stage of the script."
    echo "  -N|--niceness PRIORITY"
    echo "      Set the niceness for script."
    echo "      Default: 19"
    echo "  -v|--verbose"
    echo "      Show verbose output."
    echo ""
}

if [[ -z "$1" || $1 == "-h" || $1 == "--help" || $1 == "help" ]]; then
    run_help
    exit 0
fi

# Parse command arguments
parse_args() {
    # Parse flag arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
        -t|--harvest)
            ARGS[HARVEST]=1
            shift;;
        -f|--full)
            ARGS[FULL]=1
            shift;;
        -i|--import)
            ARGS[IMPORT]=1
            shift;;
        -X|--limit-by-delete)
            ARGS[LIMIT_BY_DELETE]="$2"
            if [[ ! "${ARGS[LIMIT_BY_DELETE]}" -gt 0 ]]; then
                echo "ERROR: -X|--limit-by-delete only accept positive integers"
                exit 1
            fi
            shift; shift ;;
        -p|--ignore-file)
            ARGS[IGNORE_FILE]=$1
            if [ ! -f "${ARGS[IGNORE_FILE]}" ]; then
                echo "ERROR: -p|--ignore-file is not a valid file"
                exit 1
            fi
            shift 2
            ;;
        -d|--vufind-harvest-dir)
            ARGS[VUFIND_HARVEST_DIR]=$( readlink -f "$2" )
            RC=$?
            if [[ "$RC" -ne 0 || ! -d "${ARGS[VUFIND_HARVEST_DIR]}" ]]; then
                echo "ERROR: -d|--vufind-harvest-dir path does not exist: $2"
                exit 1
            fi
            shift; shift ;;
        -s|--shared-harvest-dir)
            ARGS[SHARED_DIR]=$( readlink -f "$2" )
            RC=$?
            if [[ "$RC" -ne 0 || ! -d "${ARGS[SHARED_DIR]}" ]]; then
                echo "ERROR: -s|--shared-harvest-dir path does not exist: $2"
                exit 1
            fi
            shift; shift ;;
        -F|--ftp-server)
            ARGS[FTP_SERVER]="$2"
            shift; shift ;;
        -D|--ftp-dir)
            ARGS[FTP_DIR]="$2"
            shift; shift ;;
        -U|--ftp-user)
            ARGS[FTP_USER]="$2"
            shift; shift ;;
        -P|--ftp-password)
            ARGS[FTP_PASSWORD]="$2"
            shift; shift ;;
        -r|--reset-solr)
            ARGS[RESET_SOLR]=1
            shift;;
        -B|--bypass-preprocess)
            ARGS[BYPASS_PREPROCESS]=1
            shift;;
        -q|--quick)
            ARGS[QUICK]=1
            shift;;
        -N|--niceness)
            ARGS[NICENESS]="$2"
            shift; shift ;;
        -v|--verbose)
            ARGS[VERBOSE]=1
            shift;;
        *)
            echo "ERROR: Unknown flag: $1"
            exit 1
        esac
    done
}

catch_invalid_args() {
    if [[ -n "${ARGS[LIMIT_BY_DELETE]}" && "${ARGS[IMPORT]}" -ne 1 ]]; then
        echo "ERROR: The --limit-by-delete flag is only valid when --import is also set."
        exit 1
    fi
}

assert_shared_dir_writable() {
    if ! [ -w "${ARGS[SHARED_DIR]}" ]; then
        echo "ERROR: Shared storage location is not writable: ${ARGS[SHARED_DIR]}"
        exit 1
    fi
    mkdir -p "${ARGS[SHARED_DIR]}/archives/"
}

assert_vufind_harvest_dir_writable() {
    if ! [ -w "${ARGS[VUFIND_HARVEST_DIR]}" ]; then
        echo "ERROR: VuFind harvest location is not writable: ${ARGS[VUFIND_HARVEST_DIR]}"
        exit 1
    fi
}

assert_ftp_readable() {
    if ! curl "ftp://${ARGS[FTP_SERVER]}/${ARGS[FTP_DIR]}" --netrc > /dev/null 2>&1; then
        echo "ERROR: FTP harvest location is not readable: ${ARGS[FTP_SERVER]}"
        exit 1
    fi
}

create_netrc_if_needed() {
  WRITE=0
  if [[ -f ~/.netrc ]]; then
    if ! grep ~/.netrc -e "machine ${ARGS[FTP_SERVER]}"; then
      WRITE=1
    fi
  else
    WRITE=1
    verbose "Creating ~/.netrc file"
    touch ~/.netrc
    chmod 600 ~/.netrc
  fi
  if [[ WRITE -eq 1 ]]; then
    verbose "Adding content to ~/.netrc file"
    {
      echo "machine ${ARGS[FTP_SERVER]}"
      echo "login ${ARGS[FTP_USER]}"
      echo "password ${ARGS[FTP_PASSWORD]}"
    } >> ~/.netrc
  fi
}

# Print message if verbose is enabled
verbose() {
    if [[ "${ARGS[VERBOSE]}" -eq 1 ]]; then
        _log "$*"
    fi
}

# Always print the message
error() {
    _log "ERROR: " "$*"
    exit 1
}

_log() {
    LOG_TS=$(date +%Y-%m-%d\ %H:%M:%S)
    MSG="[${LOG_TS}] $*"
    echo "${MSG}"
    echo "${MSG}" >> "$LOG_FILE"

}

verbose_inline() {
    MSG="$1"
    if [[ "${ARGS[VERBOSE]}" -eq 1 ]]; then
        echo -n -e "${MSG}"
    fi
    echo -n -e "${MSG}" >> "$LOG_FILE"
}

#####
# Start a timed countdown (allowing user to cancel)
#  $1 => (Optional) String message to display before countdown; default: "Proceeding in:"
#  $2 => (Optional) Integer number of seconds to countdown from; default: 5
countdown() {
    if [[ "${ARGS[QUICK]}" -eq 1 ]]; then return; fi
    CD_CNT="${1:-5}"
    CD_MSG="${2:-Proceeding in:}"
    verbose_inline "${CD_MSG}"
    while [[ "$CD_CNT" -gt 0 ]]; do
        verbose_inline " ${CD_CNT}";
        sleep 1.1
        (( CD_CNT -= 1 ))
    done
    verbose_inline "\n"
}

# Print the last modified time as epoch seconds, or 0 if not a valid/accessible file
last_modified() {
    if [[ ! -f "$1" ]]; then
        echo "0"
    else
        stat --format=%Y "$1"
    fi
}

archive_harvest() {
    assert_shared_dir_writable
    verbose "Creating archive of latest harvest"

    ARCHIVE_TS=$(date +%Y%m%d_%H%M%S)
    ARCHIVE_FILE="${ARGS[SHARED_DIR]}/archives/archive_${ARCHIVE_TS}.tar.gz"
    pushd "${ARGS[VUFIND_HARVEST_DIR]}/" > /dev/null 2>&1 || exit 1
    declare -a ARCHIVE_LIST
    while read -r FILE; do
        ARCHIVE_LIST+=("$FILE")
    done < <( find ./ -mindepth 1 -maxdepth 1 -name '*.MRC' )

    # Archive all MRC files
    if [[ "${#ARCHIVE_LIST[@]}" -gt 0 ]]; then
        verbose "Archiving ${#ARCHIVE_LIST[@]} harvest files."
        countdown 5
        if ! tar -czvf "$ARCHIVE_FILE" "${ARCHIVE_LIST[@]}"; then
            echo "ERROR: Could not archive harvest files into ${ARCHIVE_FILE}"
            exit 1
        fi
    fi
    popd > /dev/null 2>&1 || exit 1
}

clear_harvest_files() {
    countdown 5
    find "${1}" -mindepth 1 -maxdepth 1 -name '*.MRC' -delete
}

is_bigger_than_50mb() {
    # shellcheck disable=SC2012
    if [[ "$(ls -s --block-size=1048576 "$1" | cut -d' ' -f1)" -ge 50 ]]; then
        return 0
    else
        return 1
    fi
}

# Determines if a file matches any of the provided ignore substring patterns
# Returns 0 if there is a match found.
# Returns 1 when the file is not found in any of the ignored substrings
is_ignored() {
    FILE="${1}"
    for SUBSTR in "${IGNORE_SUBSTR[@]}"; do
        if [[ "${FILE}" == *"${SUBSTR}"* ]]; then
            return 0
        fi
    done
    return 1
}

is_zip_or_marc_file() {
  if [[ "${1}" == *.m*c || "${1}" == *.zip || "${1}" == *.M*C || "${1}" == *.ZIP ]]; then
    return 0
  fi
  return 1
}


# Returns 0 when the file exists with the same size
# Returns 1 when the file is missing or the file has a different size
# than the version we have downloaded already
is_missing_or_different_size() {
    FILE="${1}"
    LOCAL_FILE_PATH=""

    if [[ "${FILE}" == *.zip ]]; then
      EXTENSION=".zip"
    elif [[ "${FILE}" == *.marc || "${FILE}" == *.mrc ]]; then
      EXTENSION=".marc"
    else
      EXTENSION=""
    fi

    # Generate the "clean" file name so it matches what we'd have stored locally
    CLEAN_NAME="$(echo "${FILE}" | sed -e "s/\.marc$//" -e "s/\.mrc$//" -e "s/\.zip$//" -e "s/ /_/g")"${EXTENSION}

    if [ -f "${ARGS[VUFIND_HARVEST_DIR]}/${FILE}" ]; then
        LOCAL_FILE_PATH="${ARGS[VUFIND_HARVEST_DIR]}/${CLEAN_NAME}"
    elif [ -f "${ARGS[VUFIND_HARVEST_DIR]}/processed/${CLEAN_NAME}" ]; then
        LOCAL_FILE_PATH="${ARGS[VUFIND_HARVEST_DIR]}/processed/${CLEAN_NAME}"
    else
        # File path is missing and we need to download it
        verbose "Could not find ${FILE} (${CLEAN_NAME}) locally"
        return 1
    fi

    LOCAL_SIZE=$(stat -c %s "${LOCAL_FILE_PATH}")
    REMOTE_SIZE=$(curl "ftp://${ARGS[FTP_SERVER]}/${ARGS[FTP_DIR]}/${FILE// /%20}" --netrc -sI | grep -i Content-Length | awk '{print $2}' | sed -e 's/[[:space:]]*$//')
    if [ "$LOCAL_SIZE" -ne "$REMOTE_SIZE" ]; then
        verbose "${FILE} (${LOCAL_SIZE}) exists locally but is a different size from the remote (${REMOTE_SIZE}). Removing local copy first"
        if ! rm "${LOCAL_FILE_PATH}"; then
            error "Could not remove original ${LOCAL_FILE_PATH} before downloading newer version on remote"
        fi
        return 1
    fi
    return 0
}

# Perform an harvest of HLM Records
harvest() {
    assert_vufind_harvest_dir_writable
    assert_shared_dir_writable
    create_netrc_if_needed
    assert_ftp_readable

    # Read the ignore pattern file into the array
    # Ignoring shellcheck not recognizing EOF
    # shellcheck disable=SC2034
    touch "${ARGS[IGNORE_FILE]}"
    IGNORE_SUBSTR=()
    while IFS= read -r line; do
        IGNORE_SUBSTR+=("$line")
    done < "${ARGS[IGNORE_FILE]}"

    if [[ "${ARGS[FULL]}" -eq 1 ]]; then
        archive_harvest
        verbose "Clearing VuFind harvest directory for new full harvest."
        clear_harvest_files "${ARGS[VUFIND_HARVEST_DIR]}/"

    fi

    cd "${ARGS[VUFIND_HARVEST_DIR]}" || error "Failed to cd to \"${ARGS[VUFIND_HARVEST_DIR]}\""
    # Check FTP server to compare files for ones we don't have
    while IFS= read -r AUTH_FILE
    do
        # If it is not an ignored pattern, not in the shared storage, and it is a MARC or zip file, then get it
        if ! is_ignored "${AUTH_FILE}" && is_zip_or_marc_file "${AUTH_FILE}" && ! is_missing_or_different_size "${AUTH_FILE}"; then
            verbose "Getting ${AUTH_FILE}"
            if ! wget "ftp://${ARGS[FTP_SERVER]}/${ARGS[FTP_DIR]}/${AUTH_FILE}" > /dev/null 2>&1; then
                error "Failed to retrieve ${AUTH_FILE} from FTP server"
            fi

            # Extract the files if it is a zip
            if [[ "${AUTH_FILE}" == *.zip ]]; then
                mkdir -p "${ARGS[VUFIND_HARVEST_DIR]}/tmp"
                unzip -qq "${AUTH_FILE}" -d "${ARGS[VUFIND_HARVEST_DIR]}/tmp"
                for F in tmp/* ; do if [[ ${F} == *.MRC ]]; then mv "${F}" "${AUTH_FILE%.zip}"_"${F#tmp/}"; fi done
                rm -rf "${ARGS[VUFIND_HARVEST_DIR]}/tmp"
                # Move the zip file to the processed directory since we don't need it for anything else
                mv "${AUTH_FILE}" "${ARGS[VUFIND_HARVEST_DIR]}/processed/"
            fi

            # Remove the files we don't currently import
            find "${ARGS[VUFIND_HARVEST_DIR]}" -maxdepth 1 \( -name "*FAST*" -o -name "*MESH.GENRE*" -o -name "*MESH.NAME*" \
                -o -name "*_NAME.CHG*" -o -name "*_NAME.NEW*" \) -delete
        fi
    done < <(curl "ftp://${ARGS[FTP_SERVER]}/${ARGS[FTP_DIR]}" --netrc -l -s)
    cd - || error "Failed to cd to previous dir"
}

# Perform VuFind batch import of HLM records
import() {
    assert_vufind_harvest_dir_writable

    verbose "Starting import..."

    if [[ -n "${ARGS[LIMIT_BY_DELETE]}" ]]; then
        verbose "Will only import ${ARGS[LIMIT_BY_DELETE]} XML files; others will be deleted."
        countdown 5
        # Delete excess files beyond the provided limit from the VUFIND_HARVEST_DIR prior to import
        FOUND_COUNT=0
        while read -r FILE; do
            (( FOUND_COUNT += 1 ))
            if [[ "${FOUND_COUNT}" -gt "${ARGS[LIMIT_BY_DELETE]}" ]]; then
                rm "$FILE"
            fi
        done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name '*.MRC')
    else
        countdown 5
    fi

    # Create required sub directories in case they don't already exist on the container
    mkdir -p "${ARGS[VUFIND_HARVEST_DIR]}/processed"

    # If there are existing part files from a prior run, clean them up
    verbose "Cleaning up leftover part files from prior runs (if any)"
    find "${ARGS[VUFIND_HARVEST_DIR]}/" -name "*-[[:digit:]]*.xml" -not -name '*.[[:digit:]]*.xml' -delete

    # Convert each file from .MRC to .xml
    verbose "Pre-processing files to convert from .MRC to .xml"
    while read -r FILE; do
        DIR_PATH=$(dirname "$FILE")
        FILENAME=$(basename "$FILE")
        filename_without_ext="${FILENAME%.*}"
        modified_filename="${filename_without_ext//./_}.xml"
        NEW_FILE_PATH="${DIR_PATH}/${modified_filename}"
        verbose "marc2xml ${FILE} > ${NEW_FILE_PATH}"
        if ! marc2xml "${FILE}" > "${NEW_FILE_PATH}"; then
            error "Failed to convert MARC -> XML ($FILE)"
        fi
        mv "${FILE}" "${ARGS[VUFIND_HARVEST_DIR]}/processed/"
    done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name '*.MRC')

    # Pre-process xml files to split contents by type placing each in separate sub-dirs
    if [[ "${ARGS[BYPASS_PREPROCESS]}" -eq 0 ]]; then
        slice_marc_files
    fi

    # Import each tag with the appropriate property file
    for TAG in "${TAGS[@]}"; do
        # Determine property file for current tag
        PROP_FILE="" # safe to have no default since TAG can't be empty
        case ${TAG} in
            100)
                PROP_FILE="marc_auth_fast_personal.properties"
                ;;
            110)
                PROP_FILE="marc_auth_fast_corporate.properties"
                ;;
            111)
                PROP_FILE="marc_auth_fast_meeting.properties"
                ;;
            130)
                PROP_FILE="marc_auth_fast_title.properties"
                ;;
            150)
                PROP_FILE="marc_auth_fast_topical.properties"
                ;;
            151)
                PROP_FILE="marc_auth_fast_geographic.properties"
                ;;
            155)
                PROP_FILE="marc_auth_fast_formgenre.properties"
                ;;
        esac
        verbose "Importing files for tag ${TAG} with property file: ${PROP_FILE}"

        while read -r FILE; do
            if [[ "${ARGS[VERBOSE]}" -eq 1 ]]; then
                if ! "$VUFIND_HOME"/import-marc-auth.sh "${FILE}" ${PROP_FILE} | tee "$LOG_FILE"; then
                    error "Batch import failed for ${FILE}."
                fi
            else
                if "$VUFIND_HOME"/import-marc-auth.sh "${FILE}" "${PROP_FILE}" >> "$LOG_FILE" 2>&1; then
                    error "Batch import failed for ${FILE}."
                fi
            fi
            if ! mv "${FILE}" "${ARGS[VUFIND_HARVEST_DIR]}"/processed/"$(basename "${FILE}")"; then
                error "Failed to moved imported file (${FILE}) to processed directory"
            fi
        done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name "*${TAG}.xml" | sort)
    done

    verbose "Completed batch import"

    # We don't get deletions from Backstage, eventually we will get them from catalogers
    verbose "Processing delete records from harvest."
    countdown 5
    if [[ "${ARGS[VERBOSE]}" -eq 1 ]]; then
        if ! /usr/local/vufind/harvest/batch-delete.sh authority | tee "$LOG_FILE"; then
            error "Batch delete script failed with code: $?"
        fi
    else
        if ! /usr/local/vufind/harvest/batch-delete.sh authority >> "$LOG_FILE"; then
            error "Batch delete script failed with code: $?"
        fi
    fi
    verbose "Completed processing records to be deleted."
}

slice_marc_files() {
    verbose "Checking for Saxon"
    if [[ ! -f "/usr/share/java/Saxon-HE.jar" ]]; then
        error "Failed to locate saxon at /usr/share/java/Saxon-HE.jar"
    fi

    verbose "Pre-processing files to split into parts by tag attribute"
    while read -r FILE; do
        # Split each file into small chunks so we don't run out of memory
        if is_bigger_than_50mb "${FILE}"; then
            SEARCH=${FILE%".xml"}
            SEARCH=${SEARCH#"${ARGS[VUFIND_HARVEST_DIR]}/"}
            verbose "Splitting ${FILE} into 50MB chunks"
            xml_split -s 50MB "${FILE}"

            # Now split each part into separate files based on the tag attribute
            verbose "Splitting by tag for files matching: ${SEARCH}*-[[:digit:]]*.xml"
            while read -r PART_FILE; do
                split_by_tag "${PART_FILE}"
            done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name "${SEARCH}*-[[:digit:]]*.xml")

            # Finally, remove the part files
            verbose "Cleaning up the un-tagged part files"
            CMD_ARRAY=(
                find "${ARGS[VUFIND_HARVEST_DIR]}/"
                -name "${SEARCH}*.xml"
                -not -name '*.[[:digit:]]*.xml'
                -delete
            )

            if ! "${CMD_ARRAY[@]}"; then
                error "Failed to remove un-tagged part files for ${SEARCH}. Command: ${CMD}"
            fi
        else
            # We only need to split each part into separate files based on the tag attribute
            split_by_tag "${FILE}"

            # Remove the non-tag files
            verbose "Cleaning up non tag file ${FILE}"
            rm "${FILE}"
        fi

    done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name '*.xml' -not -name '*.[[:digit:]]*.xml')
    # Only pre-process the unprocessed files (the ones without [name].[digit].xml)

   verbose "Completed pre-processing"
}

split_by_tag() {
    SOURCE_FILE="$1"

    # Split each part into separate files based on the tag attribute
    for TAG in "${TAGS[@]}"; do
        NEW_PATH=${SOURCE_FILE%.xml}.${TAG}.xml

        verbose "Splitting on tag ${TAG} with ${SOURCE_FILE} to ${NEW_PATH}"
        CMD_ARRAY=(
            java -cp /usr/share/java/Saxon-HE.jar
            net.sf.saxon.Query
            -s:"${SOURCE_FILE}"
            -qs:"//*[local-name()='record'][*[local-name()='datafield'][@tag=\"${TAG}\"]]"
            -o:"${NEW_PATH}"
        )
        verbose "${CMD_ARRAY[@]}"
        if ! "${CMD_ARRAY[@]}"; then
            error "Failed to split out ${TAG} tag on ${SOURCE_FILE}"
        fi

        # If the file is empty, just delete it
        if [[ -f ${NEW_PATH} ]] && [[ $(wc -l < "${NEW_PATH}") -le 3 ]]; then
            verbose "Removing empty tag file ${NEW_PATH}"
            rm "${NEW_PATH}"
        fi

        # Run sed on it to wrap it in <collection>
        if [[ -f ${NEW_PATH} ]]; then
            if ! sed -i -e '1 i\<collection>' -e '$a\</collection>' "${NEW_PATH}"; then
                error "Failed to wrap ${NEW_PATH} in <collection> element"
            fi
        fi

        # Run sed on it to remove the xml tag
        if [[ -f ${NEW_PATH} ]]; then
            if ! sed -i 's|<?xml version="1.0" encoding="UTF-8"?>||g' "${NEW_PATH}"; then
                error "Failed to remove xml tag from ${NEW_PATH}"
            fi
        fi

    done
}

# Reset the authority Solr collection by clearing all records
reset_solr() {
    if [[ "${ARGS[RESET_SOLR]}" -eq 0 ]]; then
        return
    fi
    verbose "Clearing the authority Solr index."
    countdown 5
    curl "${ARGS[SOLR_URL]}/authority/update" -H "Content-type: text/xml" --data-binary '<delete><query>*:*</query></delete>'
    curl "${ARGS[SOLR_URL]}/authority/update" -H "Content-type: text/xml" --data-binary '<commit />'
    verbose "Done clearing the Solr index."
}


# Main logic for the script
main() {
    declare -g LOG_FILE
    LOG_FILE=$(mktemp)
    verbose "Logging to ${LOG_FILE}"
    verbose "Using VUFIND_HOME of ${VUFIND_HOME}"
    if ! pushd "${VUFIND_HOME}" 2> /dev/null; then
        error "Could change directory to VUFIND_HOME!"
    fi
    if [[ ${ARGS[NICENESS]} -ne 0 ]]; then
        renice -n "${ARGS[NICENESS]}" $$
    fi
    verbose "Starting processing for ${STACK_NAME}"

    if [[ "${ARGS[HARVEST]}" -eq 1 ]]; then
        harvest
    fi
    if [[ "${ARGS[RESET_SOLR]}" -eq 1 ]]; then
        reset_solr
    fi
    if [[ "${ARGS[IMPORT]}" -eq 1 ]]; then
        import
    fi

    if ! popd 2> /dev/null; then
        error "Warning: could not change directory back to prior directory"
    fi
    verbose "All processing complete!"
}

# Parse and start running
parse_args "$@"
catch_invalid_args
main
