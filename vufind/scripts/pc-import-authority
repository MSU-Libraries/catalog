#!/bin/bash

SCRIPT_NAME="$(basename "$0")"

# Set defaults
default_args() {
    declare -g -A ARGS
    ARGS[HARVEST]=0
    ARGS[FULL]=0
    ARGS[COPY_SHARED]=0
    ARGS[IMPORT]=0
    ARGS[LIMIT]=
    ARGS[LIMIT_BY_DELETE]=
    ARGS[VUFIND_HARVEST_DIR]=/usr/local/vufind/local/harvest/authority
    ARGS[FTP_SERVER]=ftp.bslw.com
    ARGS[FTP_DIR]=out/
    ARGS[FTP_USER]=${AUTH_FTP_USER}
    ARGS[FTP_PASSWORD]=${AUTH_FTP_PASSWORD}
    ARGS[SHARED_DIR]=/mnt/authority
    ARGS[SOLR_URL]="http://solr:8983/solr"
    ARGS[RESET_SOLR]=0
    ARGS[VERBOSE]=0
    ARGS[BYPASS_PREPROCESS]=0
    declare -g -a TAGS=( 100 110 111 130 150 151 155 )
}
default_args


# Script help text
runhelp() {
    echo ""
    echo "Usage: Harvest authority records from via the FTP server"
    echo "       and import that data into VuFind's authority Solr index."
    echo ""
    echo "Examples: "
    echo "   ${SCRIPT_NAME} --harvest --full --import"
    echo "     Do a full harvest from scratch and import that data"
    echo "   ${SCRIPT_NAME} --harvest --import"
    echo "     Do an update harvest with changes made since the"
    echo "     last run, and import that data"
    echo "   ${SCRIPT_NAME} --import"
    echo "     Run only a import of data that has already been"
    echo "     harvested and saved to the shared location."
    echo "   ${SCRIPT_NAME} --harvest"
    echo "     Only run the harvest, but do not proceed to import"
    echo "     the data into VuFind"
    echo ""
    echo "FLAGS:"
    echo "  -t|--harvest"
    echo "      Run an harvest into VUFIND_HARVEST_DIR. Will attempt"
    echo "      to resume from last harvest state unless -f flag given."
    echo "      On success, sync a copy of harvest files to SHARED_DIR/current/."
    echo "  -f|--full"
    echo "      Forces a reset of VUFIND_HARVEST_DIR, resulting"
    echo "      in a full harvest. Must be used with --harvest."
    echo "  -i|--import"
    echo "      Run VuFind batch import on files within VUFIND_HARVEST_DIR."
    echo "  -c|--copy-shared"
    echo "      Copy XML file(s) from SHARED_DIR back to VUFIND_HARVEST_DIR."
    echo "      Only usable when NOT running a harvest (see also --limit)."
    echo "  -l|--limit COPY_COUNT"
    echo "      Usable with --copy-shared only. This will limit the number"
    echo "      of files copied from SHARED_DIR to VUFIND_HARVEST_DIR."
    echo "  -B|--bypass-preprocess"
    echo "      Bypass the preprocessing step of this script which splits"
    echo "      the files into separate tag files. NOTE: the import will"
    echo "      still rely on the .{TAG}.xml in the filename to determine"
    echo "      what record type to import the file as."
    echo "  -X|--limit-by-delete IMPORT_COUNT"
    echo "      Usable with --batch-import only. This will limit the number"
    echo "      of files imported from VUFIND_HARVEST_DIR by deleting XML"
    echo "      import files exceeding the given count prior to importing."
    echo "  -d|--vufind-harvest-dir VUFIND_HARVEST_DIR"
    echo "      Full path to the VuFind harvest directory."
    echo "      Default: ${ARGS[VUFIND_HARVEST_DIR]}"
    echo "  -s|--shared-harvest-dir SHARED_DIR"
    echo "      Full path to the shared storage location for HLM files."
    echo "      Default: ${ARGS[SHARED_DIR]}"
    echo "  -S|--solr SOLR_URL"
    echo "      Base URL for accessing Solr (only used for --reset-solr)."
    echo "      Default: ${ARGS[SOLR_URL]}"
    echo "  -F|--ftp-server FTP_SERVER"
    echo "      FTP server that contains the authority records"
    echo "      Default: ${ARGS[FTP_SERVER]}"
    echo "  -D|--ftp-dir FTP_DIR"
    echo "      Directory on the FTP server that contains the authority records"
    echo "      Default: ${ARGS[FTP_DIR]}"
    echo "  -U|--ftp-user FTP_USER"
    echo "      User for connecting to the FTP server"
    echo "      Default: Stored in the environment variable \$AUTH_FTP_USER"
    echo "  -P|--ftp-password FTP_PASSWORD"
    echo "      Password for connecting to the FTP server"
    echo "      Default: Stored in the environment variable \$AUTH_FTP_PASSWORD"
    echo "  -r|--reset-solr"
    echo "      Clear out the authority Solr collection prior to importing."
    echo "  -v|--verbose"
    echo "      Show verbose output."
    echo ""
}

if [[ -z "$1" || $1 == "-h" || $1 == "--help" || $1 == "help" ]]; then
    runhelp
    exit 0
fi

# Parse command arguments
parse_args() {
    # Parse flag arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
        -t|--harvest)
            ARGS[HARVEST]=1
            shift;;
        -f|--full)
            ARGS[FULL]=1
            shift;;
        -c|--copy-shared)
            ARGS[COPY_SHARED]=1
            shift;;
        -i|--import)
            ARGS[IMPORT]=1
            shift;;
        -l|--limit)
            ARGS[LIMIT]="$2"
            if [[ ! "${ARGS[LIMIT]}" -gt 0 ]]; then
                echo "ERROR: -l|--limit only accept positive integers"
                exit 1
            fi
            shift; shift ;;
        -X|--limit-by-delete)
            ARGS[LIMIT_BY_DELETE]="$2"
            if [[ ! "${ARGS[LIMIT_BY_DELETE]}" -gt 0 ]]; then
                echo "ERROR: -X|--limit-by-delete only accept positive integers"
                exit 1
            fi
            shift; shift ;;
        -d|--vufind-harvest-dir)
            ARGS[VUFIND_HARVEST_DIR]=$( readlink -f "$2" )
            RC=$?
            if [[ "$RC" -ne 0 || ! -d "${ARGS[VUFIND_HARVEST_DIR]}" ]]; then
                echo "ERROR: -d|--vufind-harvest-dir path does not exist: $2"
                exit 1
            fi
            shift; shift ;;
        -s|--shared-harvest-dir)
            ARGS[SHARED_DIR]=$( readlink -f "$2" )
            RC=$?
            if [[ "$RC" -ne 0 || ! -d "${ARGS[SHARED_DIR]}" ]]; then
                echo "ERROR: -s|--shared-harvest-dir path does not exist: $2"
                exit 1
            fi
            shift; shift ;;
        -F|--ftp-server)
            ARGS[FTP_SERVER]="$2"
            shift; shift ;;
        -D|--ftp-dir)
            ARGS[FTP_DIR]="$2"
            shift; shift ;;
        -U|--ftp-user)
            ARGS[FTP_USER]="$2"
            shift; shift ;;
        -P|--ftp-password)
            ARGS[FTP_PASSWORD]="$2"
            shift; shift ;;
        -r|--reset-solr)
            ARGS[RESET_SOLR]=1
            shift;;
        -B|--bypass-preprocess)
            ARGS[BYPASS_PREPROCESS]=1
            shift;;
        -v|--verbose)
            ARGS[VERBOSE]=1
            shift;;
        *)
            echo "ERROR: Unknown flag: $1"
            exit 1
        esac
    done
}

catch_invalid_args() {
    if [[ "${ARGS[HARVEST]}" -eq 1 && "${ARGS[COPY_SHARED]}" -eq 1 ]]; then
        echo "ERROR: It is invalid to set both --harvest and --copy-shared flags."
        exit 1
    fi
    if [[ -n "${ARGS[LIMIT]}" && "${ARGS[COPY_SHARED]}" -ne 1 ]]; then
        echo "ERROR: The --limit flag is only valid when --copy-shared is also set."
        exit 1
    fi
    if [[ -n "${ARGS[LIMIT_BY_DELETE]}" && "${ARGS[IMPORT]}" -ne 1 ]]; then
        echo "ERROR: The --limit-by-delete flag is only valid when --import is also set."
        exit 1
    fi
}

assert_shared_dir_writable() {
    if ! [ -w "${ARGS[SHARED_DIR]}" ]; then
        echo "ERROR: Shared storage location is not writable: ${ARGS[SHARED_DIR]}"
        exit 1
    fi
    mkdir -p "${ARGS[SHARED_DIR]}/current/"
    mkdir -p "${ARGS[SHARED_DIR]}/archives/"
}

assert_vufind_harvest_dir_writable() {
    if ! [ -w "${ARGS[VUFIND_HARVEST_DIR]}" ]; then
        echo "ERROR: VuFind harvest location is not writable: ${ARGS[VUFIND_HARVEST_DIR]}"
        exit 1
    fi
}

assert_ftp_readable() {
    if ! curl ftp://${ARGS[FTP_SERVER]}/${ARGS[FTP_DIR]} --netrc > /dev/null 2>&1; then
        echo "ERROR: FTP harvest location is not readable: ${ARGS[FTP_SERVER]}"
        exit 1
    fi
}

create_netrc_exists_if_needed() {
  WRITE=0
  if [[ -f ~/.netrc ]]; then
    if ! grep "machine ${ARGS[FTP_SERVER]}"; then
      WRITE=1
    fi
  else
    WRITE=1
    verbose "Creating ~/.netrc file"
    touch ~/.netrc
    chmod 600 ~/.netrc
  fi
  if [[ WRITE -eq 1 ]]; then
    verbose "Adding content to ~/.netrc file"
    {
      echo "machine ${ARGS[FTP_SERVER]}"
      echo "login ${ARGS[FTP_USER]}"
      echo "password ${ARGS[FTP_PASSWORD]}"
    } >> ~/.netrc
  fi
}

# Print message if verbose is enabled
verbose() {
    LOG_TS=$(date +%Y-%m-%d\ %H:%M:%S)
    MSG="[${LOG_TS}] $1"
    if [[ "${ARGS[VERBOSE]}" -eq 1 ]]; then
        echo "${MSG}"
    fi
    echo "${MSG}" >> "$LOG_FILE"
}

verbose_inline() {
    MSG="$1"
    if [[ "${ARGS[VERBOSE]}" -eq 1 ]]; then
        echo -n -e "${MSG}"
    fi
    echo -n -e "${MSG}" >> "$LOG_FILE"
}

#####
# Start a timed countdown (allowing user to cancel)
#  $1 => (Optional) String message to display before countdown; default: "Proceeding in:"
#  $2 => (Optional) Integer number of seconds to countdown from; default: 5
countdown() {
    CD_CNT="${1:-5}"
    CD_MSG="${2:-Proceeding in:}"
    verbose_inline "${CD_MSG}"
    while [[ "$CD_CNT" -gt 0 ]]; do
        verbose_inline " ${CD_CNT}";
        sleep 1.1
        (( CD_CNT -= 1 ))
    done
    verbose_inline "\n"
}

# Print the last modified time as epoch seconds, or 0 if not a valid/accessible file
last_modified() {
    if [[ ! -f "$1" ]]; then
        echo "0"
    else
        stat --format=%Y "$1"
    fi
}

archive_current_harvest() {
    assert_shared_dir_writable
    verbose "Creating archive of latest harvest"

    ARCHIVE_TS=$(date +%Y%m%d_%H%M%S)
    ARCHIVE_FILE="${ARGS[SHARED_DIR]}/archives/archive_${ARCHIVE_TS}.tar.gz"
    pushd "${ARGS[VUFIND_HARVEST_DIR]}/" > /dev/null 2>&1 || exit 1
    declare -a ARCHIVE_LIST
    while read -r FILE; do
        ARCHIVE_LIST+=("$FILE")
    done < <( find ./ -mindepth 1 -maxdepth 1 -name '*.MRC' )

    # Archive all MRC files
    if [[ "${#ARCHIVE_LIST[@]}" -gt 0 ]]; then
        verbose "Archiving ${#ARCHIVE_LIST[@]} harvest files."
        countdown 5
        if ! tar -czvf "$ARCHIVE_FILE" "${ARCHIVE_LIST[@]}"; then
            echo "ERROR: Could not archive harvest files into ${ARCHIVE_FILE}"
            exit 1
        fi
    fi
    popd > /dev/null 2>&1 || exit 1
}

clear_harvest_files() {
    countdown 5
    find "${1}" -mindepth 1 -maxdepth 1 -name '*.MRC' -delete
}

is_bigger_than_50mb() {
    # shellcheck disable=SC2012
    if [[ "$(ls -s --block-size=1048576 "$1" | cut -d' ' -f1)" -ge 50 ]]; then
        return 0
    else
        return 1
    fi
}

# Perform an harvest of HLM Records
harvest() {
    assert_vufind_harvest_dir_writable
    assert_shared_dir_writable
    assert_ftp_readable

    if [[ "${ARGS[FULL]}" -eq 1 ]]; then
        verbose "Clearing VuFind harvest directory for new full harvest."
        clear_harvest_files "${ARGS[VUFIND_HARVEST_DIR]}/"

        archive_current_harvest

        verbose "Clearing shared current directory before we sync the new full harvest."
        clear_harvest_files "${ARGS[SHARED_DIR]}/current/"
    fi

    # Check FTP server to compare files for ones we don't have
    OLD_PWD=$(pwd)
    if ! cd ${ARGS[VUFIND_HARVEST_DIR]}; then
        verbose "Failed to change directory into the harvest dir" 1
        exit 1
    fi
    while IFS= read -r AUTH_FILE
    do
        # If it is not in the harvest dir and it is an zip file, then get it
        if [ ! -f "${ARGS[SHARED_DIR]}/current/${AUTH_FILE}" ] && [[ ${AUTH_FILE} == *.zip ]]; then
            if ! wget ftp://${ARGS[FTP_SERVER]}/${ARGS[FTP_DIR]}/"${AUTH_FILE}" > /dev/null 2>&1; then
                verbose "ERROR: Failed to retrieve ${AUTH_FILE} from FTP server" 1
                exit 1
            fi
            mkdir -p ${ARGS[VUFIND_HARVEST_DIR]}/tmp
            unzip -qq "${AUTH_FILE}" -d ${ARGS[VUFIND_HARVEST_DIR]}/tmp
            for F in tmp/* ; do if [[ ${F} == *.MRC ]]; then mv "${F}" "${AUTH_FILE%.zip}"_"${F#tmp/}"; fi done
            rm -rf ${ARGS[VUFIND_HARVEST_DIR]}/tmp

            # Remove the files we don't currently import
            find ${ARGS[VUFIND_HARVEST_DIR]} -maxdepth 1 \( -name "*FAST*" -o -name "*MESH.GENRE*" -o -name "*MESH.NAME*" \
                -o -name "*_NAME.CHG*" -o -name "*_NAME.NEW*" \) -delete
        fi
    done < <(curl ftp://${ARGS[FTP_SERVER]}/${ARGS[FTP_DIR]} --netrc -l -s)
    if ! cd "${OLD_PWD}"; then
        verbose "Failed to change directory to original path" 1
        exit 1
    fi

    verbose "Syncing harvest files to shared storage."
    if ! rsync -ai --exclude "processed" --exclude "log" --exclude ".gitkeep" "${ARGS[VUFIND_HARVEST_DIR]}"/ "${ARGS[SHARED_DIR]}/current/" > /dev/null 2>&1; then
        verbose "ERROR: Failed to sync harvest files to shared storage from ${ARGS[VUFIND_HARVEST_DIR]}" 1
        exit 1
    fi
}

# Copy MRC files back from shared dir to VuFind dir
copyback_from_shared() {
    assert_vufind_harvest_dir_writable
    verbose "Replacing any VuFind files with files from shared directory."
    countdown 5

    # Clear out any exising xml files before copying back from shared storage
    clear_harvest_files "${ARGS[VUFIND_HARVEST_DIR]}/"

    COPIED_COUNT=0
    while read -r FILE; do
        cp --preserve=timestamps "${FILE}" "${ARGS[VUFIND_HARVEST_DIR]}/"
        (( COPIED_COUNT += 1 ))
        if [[ -n "${ARGS[LIMIT]}" && "${COPIED_COUNT}" -ge "${ARGS[LIMIT]}" ]]; then
            # If limit is set, only copy the provided limit of xml files over to the VUFIND_HARVEST_DIR
            break
        fi
    done < <(find "${ARGS[SHARED_DIR]}/current/" -mindepth 1 -maxdepth 1 -name '*.MRC')
}

# Perform VuFind batch import of HLM records
import() {
    assert_vufind_harvest_dir_writable

    verbose "Starting import..."

    if [[ -n "${ARGS[LIMIT_BY_DELETE]}" ]]; then
        verbose "Will only import ${ARGS[LIMIT_BY_DELETE]} XML files; others will be deleted."
        countdown 5
        # Delete excess files beyond the provided limit from the VUFIND_HARVEST_DIR prior to import
        FOUND_COUNT=0
        while read -r FILE; do
            (( FOUND_COUNT += 1 ))
            if [[ "${FOUND_COUNT}" -gt "${ARGS[LIMIT_BY_DELETE]}" ]]; then
                rm "$FILE"
            fi
        done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name '*.MRC')
    else
        countdown 5
    fi

    # Create required sub directories in case they don't already exist on the container
    mkdir -p ${ARGS[VUFIND_HARVEST_DIR]}/processed

    # Convert each file from .MRC to .xml
    verbose "Pre-processing files to convert from .MRC to .xml"
    while read -r FILE; do
        marc2xml "${FILE}" > "${FILE%.MRC}".xml
        mv "${FILE}" ${ARGS[VUFIND_HARVEST_DIR]}/processed/
    done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name '*.MRC')

    # Pre-process xml files to split contents by type placing each in separate subdirs
    if [[ "${ARGS[BYPASS_PREPROCESS]}" -eq 0 ]]; then
        slice_marc_files
    fi

    # Import each tag with the appropriate property file
    for TAG in "${TAGS[@]}"; do
        # Determine property file for current tag
        PROP_FILE="" # safe to have no default since TAG can't be empty
        case ${TAG} in
            100)
                PROP_FILE="marc_auth_fast_personal.properties"
                ;;
            110)
                PROP_FILE="marc_auth_fast_corporate.properties"
                ;;
            111)
                PROP_FILE="marc_auth_fast_meeting.properties"
                ;;
            130)
                PROP_FILE="marc_auth_fast_title.properties"
                ;;
            150)
                PROP_FILE="marc_auth_fast_topical.properties"
                ;;
            151)
                PROP_FILE="marc_auth_fast_geographic.properties"
                ;;
            155)
                PROP_FILE="marc_auth_fast_formgenre.properties"
                ;;
        esac
        verbose "Importing files for tag ${TAG} with property file: ${PROP_FILE}"

        while read -r FILE; do
            if [[ "${ARGS[VERBOSE]}" -eq 1 ]]; then
                "$VUFIND_HOME"/import-marc-auth.sh "${FILE}" ${PROP_FILE} | tee "$LOG_FILE"
                EXIT_CODE=$?
            else
                "$VUFIND_HOME"/import-marc-auth.sh "${FILE}" "${PROP_FILE}" >> "$LOG_FILE" 2>&1
                EXIT_CODE=$?
            fi
            if [[ ${EXIT_CODE} -eq 0 ]]; then
                mv "${FILE}" "${ARGS[VUFIND_HARVEST_DIR]}"/processed/"$(basename "${FILE}")"
            else
                verbose "ERROR: Batch import failed with code: ${EXIT_CODE}" 1
                exit 1
            fi
        done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name "*${TAG}.xml" | sort)
    done

    verbose "Completed batch import"

    # We don't get deletions from Backstage, eventually we will get them from catalogers
    verbose "Processing delete records from harvest."
    countdown 5
    if [[ "${ARGS[VERBOSE]}" -eq 1 ]]; then
        if ! /usr/local/vufind/harvest/batch-delete.sh authority | tee "$LOG_FILE"; then
            verbose "ERROR: Batch delete script failed with code: $?" 1
            exit 1
        fi
    else
        if ! /usr/local/vufind/harvest/batch-delete.sh authority >> "$LOG_FILE"; then
            verbose "ERROR: Batch delete script failed with code: $?" 1
            exit 1
        fi
    fi
    verbose "Completed processing records to be deleted."

    verbose "Syncing imported files to shared storage."
    mkdir -p "${ARGS[SHARED_DIR]}/current/processed/"
    if ! rsync -ai "${ARGS[VUFIND_HARVEST_DIR]}"/processed/ "${ARGS[SHARED_DIR]}/current/processed/" > /dev/null 2>&1; then
        verbose "ERROR: Failed to sync imported files to shared storage from ${ARGS[VUFIND_HARVEST_DIR]}/processed" 1
        exit 1
    fi
}

slice_marc_files() {
    verbose "Pre-processing files to split into parts by tag attribute"
    while read -r FILE; do
        # Split each file into small chunks so we don't run out of memory
        if is_bigger_than_50mb "${FILE}"; then
            SEARCH=${FILE%".xml"}
            SEARCH=${SEARCH#"${ARGS[VUFIND_HARVEST_DIR]}/"}
            verbose "Splitting ${FILE} into 50MB chunks"
            xml_split -s 50MB "${FILE}"

            # Now split each part into separate files based on the tag attribute
            verbose "Splitting by tag for files matching: ${SEARCH}*-[[:digit:]]*.xml"
            while read -r PART_FILE; do
                split_by_tag "${PART_FILE}"
            done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name "${SEARCH}*-[[:digit:]]*.xml")

            # Next, merge the parts for each tag together
            for TAG in "${TAGS[@]}"; do
                OUT_FILE=${FILE%.xml}.${TAG}.xml
                verbose "Merging parts for tag ${TAG} back into ${OUT_FILE}"
                if ! OUTPUT=$(xml_grep --pretty_print indented --cond "collection" "${ARGS[VUFIND_HARVEST_DIR]}/${SEARCH}*.${TAG}.xml" > "${ARGS[VUFIND_HARVEST_DIR]}/${OUT_FILE}"); then
                    verbose "ERROR: Failed to merge the parts for ${TAG} back into ${OUT_FILE}. ${OUTPUT}" 1
                fi
                # Cleaning up tags to match what import script expects
                if ! OUTPUT=$(sed -i '/<collection>/d' "${ARGS[VUFIND_HARVEST_DIR]}/${OUT_FILE}"); then
                    verbose "ERROR: Failed to remove opening tag from ${OUT_FILE}. ${OUTPUT}" 1
                fi
                if ! OUTPUT=$(sed -i '/<\/collection>/d' "${ARGS[VUFIND_HARVEST_DIR]}/${OUT_FILE}"); then
                    verbose "ERROR: Failed to remove closing tag from ${OUT_FILE}. ${OUTPUT}" 1
                fi
                if ! OUTPUT=$(sed -i 's/xml_grep/collection/g' "${ARGS[VUFIND_HARVEST_DIR]}/${OUT_FILE}"); then
                    verbose "ERROR: Failed to replace generic xml_grep wrapping tag with collection tag in ${OUT_FILE}. ${OUTPUT}" 1
                fi
                # TODO Find a new way to check this, the file will no longer be empty since it will have opening and closing tags
                # If the file is empty, just delete it
                #if [[ ! -s ${OUT_FILE} ]]; then
                #    verbose "Removing empty merge file ${OUT_FILE}"
                #    rm "${OUT_FILE}"
                #fi
            done

            # Finally, remove the part files
            verbose "Cleaning up remaining unmerged part files"
            find "${ARGS[VUFIND_HARVEST_DIR]}/" -name "${SEARCH}*-[[:digit:]]*.xml" -delete

            verbose "Cleaning up original non-tag xml file: ${FILE%".xml"}"
            rm "${FILE%".xml"}"
        else
            # We only need to split each part into separate files based on the tag attribute
            split_by_tag "${FILE}"

            # Remove the non-tag files
            verbose "Cleaning up non tag file ${FILE}"
            rm "${FILE}"
        fi

    done < <(find "${ARGS[VUFIND_HARVEST_DIR]}/" -mindepth 1 -maxdepth 1 -name '*.xml')

   verbose "Completed pre-processing"
}

split_by_tag() {
    SOURCE_FILE="$1"

    # Split each part into separate files based on the tag attribute
    for TAG in "${TAGS[@]}"; do
        NEW_PATH=${SOURCE_FILE%.xml}.${TAG}.xml

        verbose "Running xpath on tag ${TAG} with ${SOURCE_FILE} to ${NEW_PATH}"
        if ! xpath -q -e "//record[datafield[@tag=\"${TAG}\"]]" "${SOURCE_FILE}" > "${NEW_PATH}"; then
            verbose "Failed to split out ${TAG} tag on ${SOURCE_FILE}" 1
            exit 1
        fi

        # If the file is empty, just delete it
        if [[ -f ${NEW_PATH} ]] && [[ ! -s ${NEW_PATH} ]]; then
            verbose "Removing empty tag file ${NEW_PATH}"
            rm "${NEW_PATH}"
        fi

        # Run sed on it to wrap it in <collection>
        if [[ -f ${NEW_PATH} ]]; then
            if ! sed -i -e '1 i\<collection>' -e '$a\</collection>' "${NEW_PATH}"; then
                verbose "Failed to wrap ${NEW_PATH} in <collection> element" 1
                exit 1
            fi
        fi

    done
}

# Reset the authority Solr collection by clearing all records
reset_solr() {
    if [[ "${ARGS[RESET_SOLR]}" -eq 0 ]]; then
        return
    fi
    verbose "Clearing the authority Solr index."
    countdown 5
    curl "${ARGS[SOLR_URL]}/authority/update" -H "Content-type: text/xml" --data-binary '<delete><query>*:*</query></delete>'
    curl "${ARGS[SOLR_URL]}/authority/update" -H "Content-type: text/xml" --data-binary '<commit />'
    verbose "Done clearing the Solr index."
}


# Main logic for the script
main() {
    declare -g LOG_FILE
    LOG_FILE=$(mktemp)
    verbose "Logging to ${LOG_FILE}"
    verbose "Using VUFIND_HOME of ${VUFIND_HOME}"
    if ! pushd "${VUFIND_HOME}" 2> /dev/null; then
        verbose "Could change directory to VUFIND_HOME!" 1
        exit 1
    fi
    verbose "Starting processing for ${STACK_NAME}"

    if [[ "${ARGS[HARVEST]}" -eq 1 ]]; then
        harvest
    elif [[ "${ARGS[COPY_SHARED]}" -eq 1 ]]; then
        copyback_from_shared
    fi
    if [[ "${ARGS[RESET_SOLR]}" -eq 1 ]]; then
        reset_solr
    fi
    if [[ "${ARGS[IMPORT]}" -eq 1 ]]; then
        import
    fi

    if ! popd 2> /dev/null; then
        verbose "Warning: could not change directory back to prior directory" 1
        exit 0
    fi
    verbose "All processing complete!"
}

# Parse and start running
parse_args "$@"
catch_invalid_args
main
